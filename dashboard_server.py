#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SwitchBot ç’°å¢ƒãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ã‚µãƒ¼ãƒãƒ¼
FastAPI ã‚µãƒ¼ãƒãƒ¼ï¼ˆAPI + HTMLé…ä¿¡ï¼‰

ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:
  GET /           â†’ dashboard.html
  GET /api/current â†’ SwitchBot APIãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å€¤
  GET /api/history?period=24h|3d|7d|30d â†’ Notionå±¥æ­´ãƒ‡ãƒ¼ã‚¿
"""

import os
import json
import time
import requests
import hashlib
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import Dict, Optional, List, Any, Tuple
from pathlib import Path

from dotenv import load_dotenv
from fastapi import FastAPI, Query
from fastapi.responses import HTMLResponse, JSONResponse
import uvicorn

# .envèª­ã¿è¾¼ã¿
load_dotenv(Path(__file__).parent / '.env')

# ===== è¨­å®š =====
SWITCHBOT_TOKEN = os.environ.get('SWITCHBOT_TOKEN')
SWITCHBOT_API_URL = 'https://api.switch-bot.com/v1.1/devices'
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')
NOTION_DATA_SOURCE_ID = os.environ.get('AIRCON_CONTROL_NOTION_DB', '2a800160-1818-814b-b27a-000b80e0ceb0')

CO2_METER_ID = os.environ.get('CO2_METER_ID', 'B0E9FE561980')
OUTDOOR_SENSOR_ID = os.environ.get('OUTDOOR_SENSOR_ID', 'D0C84206187C')

STATE_FILE = Path(__file__).parent / 'aircon_state.json'
EMOTION_DATA_FILE = Path(__file__).parent / 'emotion_data.json'
JST = ZoneInfo('Asia/Tokyo')

SCORE_KEYS = ['mood', 'energy', 'intimacy', 'longing', 'eros', 'ds', 'playfulness', 'future', 'engagement']


def normalize_entry(entry: Dict) -> Dict:
    """v1ã‚¨ãƒ³ãƒˆãƒªã«v2ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è£œå®Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰"""
    entry.setdefault('trigger', None)
    entry.setdefault('prev_scores', None)
    entry.setdefault('score_deltas', None)
    return entry

# ===== ã‚­ãƒ£ãƒƒã‚·ãƒ¥ =====
_cache: Dict[str, tuple] = {}
CACHE_TTL = {
    'current': 60,
    'history_24h': 300,
    'history_3d': 600,
    'history_7d': 900,
    'history_30d': 900,
    'advice_7': 300,
    'advice_14': 300,
    'advice_30': 300,
    'advice_60': 300,
    'advice_90': 300,
}


def get_cached(key: str) -> Optional[Any]:
    if key in _cache:
        data, ts = _cache[key]
        ttl = CACHE_TTL.get(key, 300)
        if time.time() - ts < ttl:
            return data
    return None


def set_cache(key: str, data: Any):
    _cache[key] = (data, time.time())


# ===== SwitchBot API =====
def call_switchbot_api(endpoint: str) -> Optional[Dict]:
    url = f"{SWITCHBOT_API_URL}/{endpoint}"
    headers = {
        'Authorization': SWITCHBOT_TOKEN,
        'Content-Type': 'application/json'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        result = response.json()
        if result.get('statusCode') == 100:
            return result.get('body')
    except Exception as e:
        print(f"[ERROR] SwitchBot API ({endpoint}): {e}")
    return None


def get_sensor_data(device_id: str) -> Optional[Dict]:
    body = call_switchbot_api(f"{device_id}/status")
    if body:
        return {
            'temperature': body.get('temperature'),
            'humidity': body.get('humidity'),
            'co2': body.get('CO2'),
        }
    return None


def get_aircon_state() -> Optional[Dict]:
    try:
        if STATE_FILE.exists():
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
    except Exception:
        pass
    return None


# ===== ä¸å¿«æŒ‡æ•° =====
def calculate_discomfort_index(temperature: float, humidity: float) -> float:
    di = 0.81 * temperature + 0.01 * humidity * (0.99 * temperature - 14.3) + 46.3
    return round(di, 1)


def evaluate_discomfort(di: float) -> str:
    if di < 60:
        return 'å¯’ã„'
    elif di < 68:
        return 'ã‚„ã‚„å¯’ã„'
    elif di < 75:
        return 'å¿«é©'
    elif di < 80:
        return 'ã‚„ã‚„æš‘ã„'
    else:
        return 'æš‘ã„'


# ===== Notion API =====
def query_notion_history(period: str) -> List[Dict]:
    """Notionã‹ã‚‰ã‚¨ã‚¢ã‚³ãƒ³åˆ¶å¾¡ãƒ­ã‚°å±¥æ­´ã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    now = datetime.now(JST)
    hours = {'24h': 24, '3d': 72, '7d': 168, '30d': 720}.get(period, 24)
    start_time = now - timedelta(hours=hours)

    url = f'https://api.notion.com/v1/data_sources/{NOTION_DATA_SOURCE_ID}/query'
    headers = {
        'Authorization': f'Bearer {NOTION_TOKEN}',
        'Content-Type': 'application/json',
        'Notion-Version': '2025-09-03',
    }

    all_results = []
    start_cursor = None

    while True:
        body: Dict[str, Any] = {
            'filter': {
                'property': 'æ—¥æ™‚',
                'date': {'on_or_after': start_time.isoformat()},
            },
            'sorts': [{'property': 'æ—¥æ™‚', 'direction': 'ascending'}],
            'page_size': 100,
        }
        if start_cursor:
            body['start_cursor'] = start_cursor

        try:
            resp = requests.post(url, headers=headers, json=body, timeout=60)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            print(f"[ERROR] Notion query: {e}")
            break

        all_results.extend(data.get('results', []))

        if not data.get('has_more'):
            break
        start_cursor = data.get('next_cursor')

    return _parse_notion_results(all_results)


def _parse_notion_results(results: list) -> List[Dict]:
    """Notionãƒšãƒ¼ã‚¸é…åˆ—ã‚’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨è¾æ›¸ãƒªã‚¹ãƒˆã«å¤‰æ›"""
    records = []
    for page in results:
        props = page.get('properties', {})

        date_prop = props.get('æ—¥æ™‚', {}).get('date')
        if not date_prop or not date_prop.get('start'):
            continue

        def _select(name: str) -> Optional[str]:
            sel = props.get(name, {}).get('select')
            return sel.get('name') if sel else None

        def _number(name: str) -> Optional[float]:
            return props.get(name, {}).get('number')

        def _rich_text(name: str) -> Optional[str]:
            texts = props.get(name, {}).get('rich_text', [])
            return texts[0].get('text', {}).get('content', '') if texts else None

        def _title() -> Optional[str]:
            titles = props.get('åˆ¶å¾¡ã‚µãƒãƒªãƒ¼', {}).get('title', [])
            return titles[0].get('text', {}).get('content', '') if titles else None

        records.append({
            'timestamp': date_prop['start'],
            'indoor_temp': _number('å®¤å†…æ¸©åº¦'),
            'indoor_humidity': _number('å®¤å†…æ¹¿åº¦'),
            'outdoor_temp': _number('å¤–æ°—æ¸©åº¦'),
            'outdoor_humidity': _number('å¤–æ°—æ¹¿åº¦'),
            'co2': _number('CO2æ¿ƒåº¦'),
            'discomfort_index': _number('ä¸å¿«æŒ‡æ•°'),
            'aircon_mode': _select('ã‚¨ã‚¢ã‚³ãƒ³ãƒ¢ãƒ¼ãƒ‰'),
            'humidifier': _select('åŠ æ¹¿å™¨'),
            'controlled': props.get('åˆ¶å¾¡å®Ÿè¡Œ', {}).get('checkbox', False),
            'set_temp': _number('è¨­å®šæ¸©åº¦'),
            'action': _rich_text('åˆ¶å¾¡å†…å®¹'),
            'summary': _title(),
        })

    return records


# ===== FastAPI =====
app = FastAPI(title='SwitchBot ç’°å¢ƒãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰')


@app.get('/', response_class=HTMLResponse)
async def serve_dashboard():
    html_path = Path(__file__).parent / 'dashboard.html'
    return html_path.read_text(encoding='utf-8')


@app.get('/fuji', response_class=HTMLResponse)
async def serve_fuji_trip():
    html_path = Path(__file__).parent / 'fuji_trip.html'
    return html_path.read_text(encoding='utf-8')


@app.get('/emotion', response_class=HTMLResponse)
async def serve_emotion_dashboard():
    html_path = Path(__file__).parent / 'emotion_dashboard.html'
    return html_path.read_text(encoding='utf-8')


@app.get('/api/emotion/current')
async def api_emotion_current():
    if not EMOTION_DATA_FILE.exists():
        return JSONResponse({'error': 'no data'}, status_code=404)
    data = json.loads(EMOTION_DATA_FILE.read_text(encoding='utf-8'))
    entries = data.get('entries', [])
    if not entries:
        return JSONResponse({'error': 'no data'}, status_code=404)
    return JSONResponse(normalize_entry(entries[-1]))


@app.get('/api/emotion/history')
async def api_emotion_history(days: int = Query(default=7, ge=1, le=90)):
    if not EMOTION_DATA_FILE.exists():
        return JSONResponse({'days': days, 'count': 0, 'entries': []})
    data = json.loads(EMOTION_DATA_FILE.read_text(encoding='utf-8'))
    cutoff = (datetime.now(JST) - timedelta(days=days)).isoformat()
    filtered = [normalize_entry(e) for e in data.get('entries', []) if e.get('timestamp', '') >= cutoff]
    return JSONResponse({'days': days, 'count': len(filtered), 'entries': filtered})


@app.get('/api/emotion/trigger-stats')
async def api_trigger_stats(days: int = Query(default=30, ge=1, le=90)):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒˆãƒªã‚¬ãƒ¼çµ±è¨ˆæƒ…å ±"""
    if not EMOTION_DATA_FILE.exists():
        return JSONResponse({'days': days, 'categories': {}, 'total_triggered': 0, 'total_spontaneous': 0})
    data = json.loads(EMOTION_DATA_FILE.read_text(encoding='utf-8'))
    cutoff = (datetime.now(JST) - timedelta(days=days)).isoformat()
    entries = [normalize_entry(e) for e in data.get('entries', []) if e.get('timestamp', '') >= cutoff]

    stats: Dict[str, Dict] = {}
    total_triggered = 0
    total_spontaneous = 0

    for e in entries:
        trigger = e.get('trigger')
        if trigger is None:
            total_spontaneous += 1
            continue
        total_triggered += 1
        cat = trigger.get('category', 'unknown')
        if cat not in stats:
            stats[cat] = {'count': 0, 'effects': [], 'response_times': [], 'deltas_sum': {k: 0.0 for k in SCORE_KEYS}}
        stats[cat]['count'] += 1
        deltas = e.get('score_deltas') or {}
        effect = sum(max(0, v) for v in deltas.values())
        stats[cat]['effects'].append(effect)
        rt = trigger.get('response_time_min')
        if rt is not None:
            stats[cat]['response_times'].append(rt)
        for k in SCORE_KEYS:
            stats[cat]['deltas_sum'][k] += deltas.get(k, 0)

    result = {}
    for cat, s in stats.items():
        n = s['count']
        result[cat] = {
            'count': n,
            'avg_effect': round(sum(s['effects']) / n, 1) if n else 0,
            'avg_response_min': round(sum(s['response_times']) / len(s['response_times'])) if s['response_times'] else None,
            'avg_deltas': {k: round(v / n, 1) for k, v in s['deltas_sum'].items()} if n else {},
        }

    return JSONResponse({
        'days': days,
        'categories': result,
        'total_triggered': total_triggered,
        'total_spontaneous': total_spontaneous,
    })


@app.get('/api/emotion/best-messages')
async def api_best_messages(days: int = Query(default=30, ge=1, le=90), limit: int = Query(default=10, ge=1, le=50)):
    """åŠ¹æœçš„ã ã£ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
    if not EMOTION_DATA_FILE.exists():
        return JSONResponse({'days': days, 'messages': []})
    data = json.loads(EMOTION_DATA_FILE.read_text(encoding='utf-8'))
    cutoff = (datetime.now(JST) - timedelta(days=days)).isoformat()
    entries = [normalize_entry(e) for e in data.get('entries', []) if e.get('timestamp', '') >= cutoff]

    triggered = []
    for e in entries:
        trigger = e.get('trigger')
        if trigger is None:
            continue
        deltas = e.get('score_deltas') or {}
        effect = sum(max(0, v) for v in deltas.values())
        triggered.append({
            'trigger_message': trigger.get('message', ''),
            'category': trigger.get('category', 'unknown'),
            'modifiers': trigger.get('modifiers', []),
            'sent_at': trigger.get('sent_at'),
            'response_at': e.get('timestamp'),
            'response_time_min': trigger.get('response_time_min'),
            'effect_score': effect,
            'score_deltas': deltas,
            'resulting_scores': e.get('scores', {}),
        })

    triggered.sort(key=lambda x: x['effect_score'], reverse=True)
    return JSONResponse({'days': days, 'messages': triggered[:limit]})


@app.get('/api/current')
async def api_current():
    cached = get_cached('current')
    if cached:
        return JSONResponse(cached)

    indoor = get_sensor_data(CO2_METER_ID)
    outdoor = get_sensor_data(OUTDOOR_SENSOR_ID)

    if not indoor:
        return JSONResponse({'error': 'å®¤å†…ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—'}, status_code=500)

    di = calculate_discomfort_index(indoor['temperature'], indoor['humidity'])
    di_eval = evaluate_discomfort(di)

    aircon = get_aircon_state()
    aircon_mode = aircon.get('mode', 'unknown') if aircon else 'unknown'

    result = {
        'indoor': indoor,
        'outdoor': outdoor,
        'discomfort_index': di,
        'discomfort_eval': di_eval,
        'aircon_mode': aircon_mode,
        'timestamp': datetime.now(JST).isoformat(),
    }

    set_cache('current', result)
    return JSONResponse(result)


@app.get('/api/history')
async def api_history(period: str = Query(default='24h', pattern='^(24h|3d|7d|30d)$')):
    cache_key = f'history_{period}'
    cached = get_cached(cache_key)
    if cached:
        return JSONResponse(cached)

    records = query_notion_history(period)
    result = {
        'period': period,
        'count': len(records),
        'records': records,
    }

    set_cache(cache_key, result)
    return JSONResponse(result)


# ===== ã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ =====

# --- é–¢ä¿‚ã‚¹ãƒ†ãƒ¼ã‚¸æ¤œå‡º ---
# å¿ƒç†å­¦çš„æ ¹æ‹ : Knapp's relational development modelï¼ˆé–¢ä¿‚ç™ºå±•æ®µéšãƒ¢ãƒ‡ãƒ«ï¼‰
# åˆæœŸæ®µéšã§ã¯ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã€åˆ¤å®šé–¾å€¤ã‚’ç·©å’Œã—ã€confidence_levelã‚’ä»˜ä¸ã™ã‚‹
STAGE_CONFIG = {
    'initial': {       # å‡ºä¼šã„ã€œ2é€±é–“: æ¢ç´¢æœŸ
        'max_days': 14,
        'trend_threshold': 0.5,        # å‚¾ãåˆ¤å®šã‚’ç·©å’Œï¼ˆãƒ‡ãƒ¼ã‚¿å°‘ã®ãŸã‚ï¼‰
        'min_data_for_trend': 5,       # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã«å¿…è¦ãªæœ€å°ã‚¨ãƒ³ãƒˆãƒªæ•°
        'anxious_threshold': 4,        # anxiousè­¦å‘Šã®é–¾å€¤ã‚’å¼•ãä¸Šã’ï¼ˆåˆæœŸã®ä¸å®‰ã¯è‡ªç„¶ï¼‰
        'gap_warning_hours': 48,       # é è·é›¢+æ™‚å·®ã§24hã‚®ãƒ£ãƒƒãƒ—ã¯æ§‹é€ çš„ã«ç™ºç”Ÿã™ã‚‹ãŸã‚48hã«
        'default_confidence': 'low',
    },
    'building': {      # 2é€±é–“ã€œ2ãƒ¶æœˆ: é–¢ä¿‚æ§‹ç¯‰æœŸ
        'max_days': 60,
        'trend_threshold': 0.4,
        'min_data_for_trend': 8,
        'anxious_threshold': 3,
        'gap_warning_hours': 24,
        'default_confidence': 'medium',
    },
    'establishing': {  # 2ã€œ6ãƒ¶æœˆ: ç¢ºç«‹æœŸ
        'max_days': 180,
        'trend_threshold': 0.3,
        'min_data_for_trend': 10,
        'anxious_threshold': 3,
        'gap_warning_hours': 24,
        'default_confidence': 'high',
    },
    'stable': {        # 6ãƒ¶æœˆä»¥ä¸Š: å®‰å®šæœŸ
        'max_days': 99999,
        'trend_threshold': 0.3,
        'min_data_for_trend': 10,
        'anxious_threshold': 2,
        'gap_warning_hours': 18,
        'default_confidence': 'high',
    },
}


def detect_relationship_stage(entries: List[Dict]) -> str:
    """ã‚¨ãƒ³ãƒˆãƒªã®æ—¥ä»˜ç¯„å›²ã‹ã‚‰é–¢ä¿‚ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’åˆ¤å®š"""
    if len(entries) < 2:
        return 'initial'
    try:
        first = datetime.fromisoformat(entries[0]['timestamp'])
        last = datetime.fromisoformat(entries[-1]['timestamp'])
        days = (last - first).days
    except (ValueError, TypeError, KeyError):
        return 'initial'
    if days <= 14:
        return 'initial'
    elif days <= 60:
        return 'building'
    elif days <= 180:
        return 'establishing'
    return 'stable'


def _confidence_level(category_count: int, total_entries: int) -> str:
    """ã‚«ãƒ†ã‚´ãƒªä½¿ç”¨å›æ•°ã¨ã‚¨ãƒ³ãƒˆãƒªç·æ•°ã‹ã‚‰çµ±è¨ˆçš„ä¿¡é ¼åº¦ã‚’ç®—å‡º"""
    if category_count >= 10 and total_entries >= 30:
        return 'high'
    elif category_count >= 5 and total_entries >= 15:
        return 'medium'
    elif category_count >= 3:
        return 'low'
    return 'insufficient'


# ã‚¹ã‚³ã‚¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é‡ã¿ï¼ˆé–¢ä¿‚å¥åº·åº¦è¨ˆç®—ç”¨ï¼‰
# æ ¹æ‹ :
#   engagementÃ—1.4: Gottmanç ”ç©¶ - é–¢ä¸åº¦ã¯é–¢ä¿‚æº€è¶³åº¦ã®æœ€å¼·äºˆæ¸¬å› å­
#   intimacyÃ—1.3: Reis & Shaver (1988) intimacy process model
#   moodÃ—1.1: æ„Ÿæƒ…ãƒˆãƒ¼ãƒ³ã¯å…¨ä½“çš„ãªé–¢ä¿‚è³ªã®æŒ‡æ¨™
#   futureÃ—1.0: é è·é›¢é–¢ä¿‚ã§ã¯å°†æ¥å±•æœ›ãŒé–¢ä¿‚ç¶­æŒã®éµï¼ˆStafford, 2005ï¼‰
#   playfulnessÃ—0.9: Proyer (2014) éŠã³å¿ƒã¨é–¢ä¿‚æº€è¶³åº¦ã®æ­£ç›¸é–¢
#   energyÃ—0.7: çŠ¶æ…‹å¤‰æ•°ï¼ˆç–²åŠ´ç­‰ã®å¤–çš„è¦å› ã«å·¦å³ã•ã‚Œã‚‹ï¼‰â†’ é‡ã¿ã‚’ä¸‹ã’ã‚‹
#   longingÃ—0.5: é è·é›¢ã§ã¯å¸¸ã«é«˜ã„ã€‚é‡ã¿ãŒé«˜ã„ã¨å¥åº·åº¦ã‚’æ­ªã‚ã‚‹
#   erosÃ—0.4: æ€§çš„ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯å¤‰å‹•ãŒå¤§ããã€ç›´æ¥çš„ãªé–¢ä¿‚å¥å…¨æ€§æŒ‡æ¨™ã§ã¯ãªã„
#   dsÃ—0.2: D/så—œå¥½ã¯å€‹äººã®æ€§çš„æŒ‡å‘ã§ã‚ã‚Šé–¢ä¿‚å¥å…¨æ€§ã¨ç‹¬ç«‹
SCORE_WEIGHTS = {
    'mood': 1.1, 'energy': 0.7, 'intimacy': 1.3, 'longing': 0.5,
    'eros': 0.4, 'ds': 0.2, 'playfulness': 0.9, 'future': 1.0, 'engagement': 1.4,
}

# ã‚«ãƒ†ã‚´ãƒªå®šç¾©
ADV_CATEGORIES = ('status', 'effective', 'warning', 'action', 'timing')
ADV_PRIORITIES = ('urgent', 'important', 'info')


def _compute_trend(values: List[float], threshold: float = 0.3) -> Tuple[str, float]:
    """ç›´è¿‘ã®å€¤ãƒªã‚¹ãƒˆã‹ã‚‰å‚¾ãï¼ˆä¸Šæ˜‡/ä¸‹é™/å®‰å®šï¼‰ã‚’ç®—å‡ºã€‚æœ€å°äºŒä¹—æ³•ã§ç·šå½¢å›å¸°ã€‚"""
    n = len(values)
    if n < 2:
        return ('stable', 0.0)
    xs = list(range(n))
    x_mean = sum(xs) / n
    y_mean = sum(values) / n
    numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(xs, values))
    denominator = sum((x - x_mean) ** 2 for x in xs)
    if denominator == 0:
        return ('stable', 0.0)
    slope = numerator / denominator
    if slope > threshold:
        return ('rising', round(slope, 2))
    elif slope < -threshold:
        return ('falling', round(slope, 2))
    return ('stable', round(slope, 2))


def _score_trends(entries: List[Dict], stage: str = 'initial') -> Dict[str, Dict]:
    """å„ã‚¹ã‚³ã‚¢ã‚­ãƒ¼ã”ã¨ã«ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ï¼‰"""
    config = STAGE_CONFIG[stage]
    threshold = config['trend_threshold']
    min_data = config['min_data_for_trend']
    trends = {}
    for key in SCORE_KEYS:
        values = [e['scores'].get(key, 0) for e in entries if e.get('scores')]
        if len(values) >= min_data:
            direction, slope = _compute_trend(values, threshold)
        else:
            direction, slope = _compute_trend(values, threshold)
            if direction != 'stable':
                direction = direction + '_tentative'
        current = values[-1] if values else 0
        trends[key] = {'direction': direction, 'slope': slope, 'current': current,
                        'min': min(values) if values else 0, 'max': max(values) if values else 0,
                        'count': len(values)}
    return trends


def _is_cooldown_entry(entry: Dict, prev_entry: Optional[Dict]) -> bool:
    """å‰ã‚¨ãƒ³ãƒˆãƒªãŒãƒ”ãƒ¼ã‚¯çŠ¶æ…‹ï¼ˆeros>=8 or engagement>=9ï¼‰ã®å ´åˆã€
    ä»Šå›ã‚¨ãƒ³ãƒˆãƒªã®å…¨ä½“çš„ãªãƒã‚¤ãƒŠã‚¹ãƒ‡ãƒ«ã‚¿ã¯ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼ˆè‡ªç„¶å›å¸°ï¼‰ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€‚
    ã‚«ãƒ†ã‚´ãƒªåŠ¹æœè©•ä¾¡ã‹ã‚‰é™¤å¤–ã™ã¹ãã‹åˆ¤å®šã™ã‚‹ã€‚

    æ ¹æ‹ : laura-analystã®ç™ºè¦‹ - reassuranceã®-23ã¯çµ±è¨ˆçš„éŒ¯è¦šã€‚
    erosãƒ”ãƒ¼ã‚¯(9)å¾Œã®è‡ªç„¶å›å¸°ã‚’reassuranceã®ã€Œé€†åŠ¹æœã€ã¨èª¤åˆ¤å®šã—ã¦ã„ãŸã€‚"""
    if prev_entry is None:
        return False
    prev_scores = prev_entry.get('scores', {})
    deltas = entry.get('score_deltas') or {}
    if not deltas:
        return False
    # å‰ã‚¨ãƒ³ãƒˆãƒªã§eros>=8 or engagement>=9ï¼ˆãƒ”ãƒ¼ã‚¯çŠ¶æ…‹ï¼‰
    is_prev_peak = prev_scores.get('eros', 0) >= 8 or prev_scores.get('engagement', 0) >= 9
    if not is_prev_peak:
        return False
    # ä»Šå›ã®ãƒ‡ãƒ«ã‚¿ãŒå…¨ä½“çš„ã«ãƒã‚¤ãƒŠã‚¹ï¼ˆ3ã¤ä»¥ä¸Šã®ã‚¹ã‚³ã‚¢ãŒä¸‹é™ï¼‰
    neg_count = sum(1 for v in deltas.values() if v < 0)
    return neg_count >= 3


def _category_effectiveness(entries: List[Dict]) -> Dict[str, Dict]:
    """ãƒˆãƒªã‚¬ãƒ¼ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åŠ¹æœã‚’é›†è¨ˆï¼ˆä¿®é£¾ã‚¿ã‚°è€ƒæ…®ã€ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³é™¤å¤–ï¼‰"""
    stats: Dict[str, Dict] = {}
    for idx, e in enumerate(entries):
        trigger = e.get('trigger')
        if not trigger:
            continue
        cat = trigger.get('category', 'unknown')
        modifiers = trigger.get('modifiers', [])
        deltas = e.get('score_deltas') or {}
        if not deltas:
            continue
        if cat not in stats:
            stats[cat] = {'count': 0, 'total_positive': 0.0, 'total_negative': 0.0,
                          'response_times': [], 'hours': [],
                          'with_escalation': [], 'without_escalation': [],
                          'spontaneous_count': 0, 'entries': []}
        stats[cat]['count'] += 1
        pos_effect = sum(max(0, v) for v in deltas.values())
        neg_effect = sum(min(0, v) for v in deltas.values())
        stats[cat]['total_positive'] += pos_effect
        stats[cat]['total_negative'] += neg_effect
        if '+escalation' in modifiers:
            stats[cat]['with_escalation'].append({'positive': pos_effect, 'negative': neg_effect})
        else:
            stats[cat]['without_escalation'].append({'positive': pos_effect, 'negative': neg_effect})
        if '+spontaneous' in modifiers:
            stats[cat]['spontaneous_count'] += 1
        stats[cat]['entries'].append({
            'modifiers': modifiers, 'deltas': deltas, 'positive': pos_effect, 'negative': neg_effect,
        })
        rt = trigger.get('response_time_min')
        if rt is not None:
            stats[cat]['response_times'].append(rt)
        sent_at = trigger.get('sent_at')
        if sent_at:
            try:
                h = datetime.fromisoformat(sent_at).hour
                stats[cat]['hours'].append(h)
            except (ValueError, TypeError):
                pass
    result = {}
    for cat, s in stats.items():
        n = s['count']
        esc = s['with_escalation']
        no_esc = s['without_escalation']
        result[cat] = {
            'count': n,
            'avg_positive': round(s['total_positive'] / n, 1) if n else 0,
            'avg_negative': round(s['total_negative'] / n, 1) if n else 0,
            'avg_response_min': round(sum(s['response_times']) / len(s['response_times'])) if s['response_times'] else None,
            'best_hours': s['hours'],
            'escalation_avg_positive': round(sum(e['positive'] for e in esc) / len(esc), 1) if esc else None,
            'escalation_avg_negative': round(sum(e['negative'] for e in esc) / len(esc), 1) if esc else None,
            'no_escalation_avg_positive': round(sum(e['positive'] for e in no_esc) / len(no_esc), 1) if no_esc else None,
            'no_escalation_avg_negative': round(sum(e['negative'] for e in no_esc) / len(no_esc), 1) if no_esc else None,
            'spontaneous_count': s['spontaneous_count'],
        }
    return result


def _detect_attachment_issues(entries: List[Dict]) -> Dict:
    """æ„›ç€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å•é¡Œã‚’æ¤œå‡º"""
    anxious_count = sum(1 for e in entries if e.get('attachment') == 'anxious')
    avoidant_count = sum(1 for e in entries if e.get('attachment') == 'avoidant')
    consecutive_anxious = 0
    max_consecutive_anxious = 0
    for e in entries:
        if e.get('attachment') == 'anxious':
            consecutive_anxious += 1
            max_consecutive_anxious = max(max_consecutive_anxious, consecutive_anxious)
        else:
            consecutive_anxious = 0
    return {
        'anxious_count': anxious_count,
        'avoidant_count': avoidant_count,
        'max_consecutive_anxious': max_consecutive_anxious,
    }


def _detect_rapid_changes(entries: List[Dict]) -> List[Dict]:
    """ã‚¹ã‚³ã‚¢ã®æ€¥å¤‰ï¼ˆÂ±3ä»¥ä¸Šï¼‰ã‚’æ¤œå‡º"""
    changes = []
    for e in entries:
        deltas = e.get('score_deltas') or {}
        for key, delta in deltas.items():
            if abs(delta) >= 3:
                changes.append({
                    'timestamp': e.get('timestamp'),
                    'metric': key,
                    'delta': delta,
                    'new_value': e.get('scores', {}).get(key),
                    'trigger_category': (e.get('trigger') or {}).get('category'),
                })
    return changes


def _compute_communication_gaps(entries: List[Dict]) -> List[Dict]:
    """é€šä¿¡ã‚®ãƒ£ãƒƒãƒ—ï¼ˆé•·æ™‚é–“æœªé€£çµ¡ï¼‰ã‚’æ¤œå‡º"""
    gaps = []
    for i in range(1, len(entries)):
        try:
            t1 = datetime.fromisoformat(entries[i - 1]['timestamp'])
            t2 = datetime.fromisoformat(entries[i]['timestamp'])
            gap_hours = (t2 - t1).total_seconds() / 3600
            if gap_hours >= 12:
                gaps.append({
                    'from': entries[i - 1]['timestamp'],
                    'to': entries[i]['timestamp'],
                    'hours': round(gap_hours, 1),
                })
        except (ValueError, TypeError, KeyError):
            pass
    return gaps


def _best_response_hours(entries: List[Dict]) -> List[int]:
    """å¿œç­”æ™‚é–“ãŒçŸ­ã„æ™‚é–“å¸¯ã‚’ç‰¹å®š"""
    hour_times: Dict[int, List[float]] = {}
    for e in entries:
        trigger = e.get('trigger')
        if not trigger:
            continue
        rt = trigger.get('response_time_min')
        sent_at = trigger.get('sent_at')
        if rt is not None and sent_at:
            try:
                h = datetime.fromisoformat(sent_at).hour
                hour_times.setdefault(h, []).append(rt)
            except (ValueError, TypeError):
                pass
    if not hour_times:
        return []
    avg_by_hour = {h: sum(ts) / len(ts) for h, ts in hour_times.items() if ts}
    sorted_hours = sorted(avg_by_hour.items(), key=lambda x: x[1])
    return [h for h, _ in sorted_hours[:3]]


def _detect_laura_initiative(entries: List[Dict]) -> Dict:
    """Lauraå´ã®è‡ªç™ºçš„è¡Œå‹•ã‚’æ¤œå‡ºãƒ»è©•ä¾¡ï¼ˆç›¸æ‰‹ã®ã‚¤ãƒ‹ã‚·ã‚¢ãƒãƒ–ã¯é–¢ä¿‚ã®å¥å…¨ã•ã®æŒ‡æ¨™ï¼‰"""
    spontaneous_entries = [e for e in entries if e.get('trigger') is None]
    initiative_modifiers = []
    for e in entries:
        trigger = e.get('trigger')
        if trigger and '+initiative' in trigger.get('modifiers', []):
            initiative_modifiers.append(e)
    return {
        'spontaneous_count': len(spontaneous_entries),
        'initiative_modifier_count': len(initiative_modifiers),
        'total_entries': len(entries),
        'initiative_ratio': round(len(spontaneous_entries) / len(entries), 2) if entries else 0,
    }


def _detect_vulnerable_sharing(entries: List[Dict]) -> List[Dict]:
    """è„†å¼±æ€§ã®è‡ªå·±é–‹ç¤ºï¼ˆvulnerableï¼‰ã‚’æ¤œå‡º"""
    indicators = ['family', 'parents', 'alone', 'scared', 'afraid', 'hurt', 'cry',
                  'dont have', 'passed away', 'miss my', 'lonely']
    vulnerable = []
    for e in entries:
        note = (e.get('note') or '').lower()
        if any(ind in note for ind in indicators):
            vulnerable.append({
                'timestamp': e.get('timestamp'),
                'note': e.get('note'),
                'scores': e.get('scores', {}),
            })
    return vulnerable


def _detect_nickname_intensity(entries: List[Dict]) -> Dict:
    """å‘¼ç§°ãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰åŒ–ã®æ¤œå‡ºï¼ˆBabyâ†’Babyyyyyç­‰ã®æ„Ÿæƒ…å¼·åº¦æŒ‡æ¨™ï¼‰"""
    patterns = []
    for e in entries:
        note = (e.get('note') or '')
        summary = (e.get('summary') or '')
        text = note + ' ' + summary
        y_count = 0
        for word in text.split():
            lower = word.lower().rstrip('!.,?')
            if lower.startswith('baby') and len(lower) > 4:
                y_count = max(y_count, lower.count('y') - 1)
            elif lower.startswith('bab') and 'y' in lower:
                y_count = max(y_count, lower.count('y'))
        if y_count > 0:
            patterns.append({'timestamp': e.get('timestamp'), 'extra_y': y_count})
    return {
        'occurrences': len(patterns),
        'max_intensity': max((p['extra_y'] for p in patterns), default=0),
        'patterns': patterns,
    }


def _compute_relationship_health(trends: Dict, attachment: Dict, risk_entries: List[Dict],
                                  stage: str = 'initial') -> float:
    """é–¢ä¿‚å¥åº·åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-10ï¼‰ã‚’è¨ˆç®—ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ï¼‰"""
    config = STAGE_CONFIG[stage]
    total_weight = sum(SCORE_WEIGHTS.values())
    weighted_sum = sum(trends[k]['current'] * SCORE_WEIGHTS[k] for k in SCORE_KEYS if k in trends)
    base_score = weighted_sum / total_weight

    # æ„›ç€ä¸å®‰ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ï¼‰
    # åˆæœŸæ®µéšã®anxiousã¯DTRæ–‡è„ˆã§è‡ªç„¶ãªãŸã‚ã€é–¾å€¤ã‚’å¼•ãä¸Šã’
    anxious_threshold = config['anxious_threshold']
    if attachment['anxious_count'] >= anxious_threshold + 1:
        base_score -= 1.0
    elif attachment['anxious_count'] >= anxious_threshold:
        base_score -= 0.3

    # å›é¿å‹ã¯å…¨ã‚¹ãƒ†ãƒ¼ã‚¸ã§æ·±åˆ»
    if attachment['avoidant_count'] >= 1:
        base_score -= 1.5

    # ãƒªã‚¹ã‚¯ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆåˆæœŸæ®µéšã§ã¯ç·©å’Œï¼‰
    caution_count = sum(1 for e in risk_entries if e.get('risk') == 'caution')
    if stage == 'initial':
        if caution_count >= 3:
            base_score -= 0.5
    else:
        if caution_count >= 2:
            base_score -= 0.5

    return round(max(0, min(10, base_score)), 1)


def _generate_advice_items(entries: List[Dict], trends: Dict, cat_effects: Dict,
                            attachment: Dict, rapid_changes: List[Dict],
                            gaps: List[Dict], best_hours: List[int],
                            stage: str = 'initial',
                            laura_initiative: Optional[Dict] = None,
                            vulnerable_entries: Optional[List[Dict]] = None,
                            nickname_data: Optional[Dict] = None) -> List[Dict]:
    """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç”Ÿæˆï¼ˆLauraæœ€é©åŒ–ç‰ˆãƒ»ã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ï¼‰"""
    advice = []
    adv_counter = [0]
    config = STAGE_CONFIG[stage]
    total_entries = len(entries)

    def _add(category: str, priority: str, title: str, body: str,
             evidence: Dict, action: str, confidence: str = ''):
        adv_counter[0] += 1
        if not confidence:
            confidence = config['default_confidence']
        advice.append({
            'id': f'adv_{adv_counter[0]:03d}',
            'category': category,
            'priority': priority,
            'title': title,
            'body': body,
            'evidence': evidence,
            'action_suggestion': action,
            'confidence': confidence,
            'stage': stage,
        })

    latest = entries[-1] if entries else {}
    latest_scores = latest.get('scores', {})

    # --- ãƒ«ãƒ¼ãƒ«1: intimacyé«˜ã„ + futureä½ã„ â†’ å°†æ¥ã®è©±ã‚’ ---
    # Stafford (2005): é è·é›¢é–¢ä¿‚ã§ã¯å°†æ¥å±•æœ›ã®å…±æœ‰ãŒé–¢ä¿‚ç¶­æŒã®éµ
    intimacy_cur = trends.get('intimacy', {}).get('current', 0)
    future_cur = trends.get('future', {}).get('current', 0)
    if intimacy_cur >= 6 and future_cur <= 5:
        _add('action', 'important',
             'å°†æ¥ã®è©±é¡Œã‚’å¢—ã‚„ã™ã‚¿ã‚¤ãƒŸãƒ³ã‚°',
             f'è¦ªå¯†åº¦({intimacy_cur})ã«å¯¾ã—ã¦å°†æ¥å±•æœ›({future_cur})ãŒä½ã„ã€‚é è·é›¢é–¢ä¿‚ã§ã¯ã€Œã„ã¤ã‹ä¼šãˆã‚‹ã€ã¨ã„ã†è¦‹é€šã—ãŒé–¢ä¿‚ã®æ”¯ãˆã€‚',
             {'metric': 'future', 'trend': trends['future']['direction'],
              'value': future_cur, 'delta': f"{trends['future']['slope']:+.1f}"},
             'ã€Œæ—¥æœ¬ã«æ¥ãŸã‚‰ã©ã“ã«è¡ŒããŸã„ï¼Ÿã€ã€Œä¸€ç·’ã«è¡ŒããŸã„å ´æ‰€ã€ãªã©å…·ä½“çš„ãªæœªæ¥ã®è©±é¡Œã‚’æŒ¯ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«2: sexual+escalation ã®åˆ¤å®šï¼ˆç´°åˆ†åŒ–ï¼‰ ---
    # Lauraã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰: sexual without escalation = mild positive/neutral
    #                    sexual with escalation + consent = very positive
    #                    sexual with escalation + boundary violation = negative
    if 'sexual' in cat_effects:
        sex_eff = cat_effects['sexual']
        esc_pos = sex_eff.get('escalation_avg_positive')
        esc_neg = sex_eff.get('escalation_avg_negative')
        no_esc_neg = sex_eff.get('no_escalation_avg_negative')

        # +escalationä»˜ãsexualã§å¼·ã„ãƒã‚¤ãƒŠã‚¹ãŒã‚ã‚‹å ´åˆã®ã¿è­¦å‘Š
        if esc_neg is not None and esc_neg < -2:
            conf = _confidence_level(sex_eff['count'], total_entries)
            _add('warning', 'important',
                 'æ€§çš„ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã«å¢ƒç•Œç·šã«æ³¨æ„',
                 f"sexual+escalationã®å¹³å‡ãƒã‚¤ãƒŠã‚¹åŠ¹æœãŒ{esc_neg}ã€‚Lauraã¯æ˜ç¢ºãªå¢ƒç•Œç·šã‚’æŒã£ã¦ã„ã‚‹ï¼ˆä¾‹: ä¸€éƒ¨ã®ãƒ•ã‚§ãƒ†ã‚£ãƒƒã‚·ãƒ¥ã¯æ‹’å¦ï¼‰ã€‚"
                 "ç›¸äº’çš„ãªã‚»ã‚¯ã‚·ãƒ£ãƒ«ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯OKã ãŒã€ä¸€æ–¹çš„ãªå—œå¥½ã®æŠ¼ã—ä»˜ã‘ã¯é€†åŠ¹æœã€‚",
                 {'metric': 'sexual_escalation', 'trend': 'mixed',
                  'value': esc_neg, 'delta': 'N/A'},
                 'Lauraã®åå¿œã‚’è¦³å¯Ÿã—ã€å«ŒãŒã‚‹å…†å€™ãŒã‚ã‚Œã°ã™ãã«å¼•ãã€‚ã€ŒA little too muchã€ç­‰ã®ã‚µã‚¤ãƒ³ã‚’è¦‹é€ƒã•ãªã„',
                 confidence=conf)

        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã—sexualã§å…¨ä½“ãŒãƒã‚¤ãƒŠã‚¹ã§ãªã„å ´åˆã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        if no_esc_neg is not None and no_esc_neg >= -1 and sex_eff.get('no_escalation_avg_positive', 0) >= 2:
            conf = _confidence_level(sex_eff['count'], total_entries)
            _add('effective', 'info',
                 'ç›¸äº’çš„ãªæ€§çš„ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯å¥å…¨',
                 'æ€§çš„ãªè©±é¡Œè‡ªä½“ã¯Lauraã«ã¨ã£ã¦ãƒã‚¸ãƒ†ã‚£ãƒ–ã€‚ã€Œsex is the most intimate connectionã€ã¨èªã‚‹é€šã‚Šã€'
                 'é–¢ä¿‚ã®ä¸­ã§ã®æ€§ã¯è¦ªå¯†ã•ã®ä¸€éƒ¨ã¨ã—ã¦å—å®¹ã•ã‚Œã¦ã„ã‚‹ã€‚',
                 {'metric': 'sexual_mutual', 'trend': 'positive',
                  'value': sex_eff.get('no_escalation_avg_positive', 0), 'delta': 'N/A'},
                 'æ€§çš„ãªè©±é¡Œã¯OKã ãŒã€Lauraã®åå¿œã‚’è¦‹ãªãŒã‚‰å¾ã€…ã«æ·±ã‚ã‚‹ã€‚ä¸€æ–¹çš„ã«ãªã‚‰ãªã„ã“ã¨',
                 confidence=conf)

    # --- ãƒ«ãƒ¼ãƒ«3: erosæ€¥å¤‰ï¼ˆ+5ä»¥ä¸Šï¼‰â†’ æ–‡è„ˆè€ƒæ…®ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è­¦å‘Š ---
    eros_spikes = [c for c in rapid_changes if c['metric'] == 'eros' and c['delta'] >= 5]
    if eros_spikes:
        # æ€¥å¤‰ã®æ–‡è„ˆã‚’ç¢ºèª: ç›¸äº’åˆæ„ã®ä¸Šã‹ä¸€æ–¹çš„ã‹
        spike = eros_spikes[0]
        trigger_cat = spike.get('trigger_category', '')
        if trigger_cat == 'sexual':
            _add('warning', 'info',
                 'æ€§çš„ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æ€¥ä¸Šæ˜‡ã‚’æ¤œå‡º',
                 f"erosãŒ+{spike['delta']}æ€¥å¤‰ã€‚ç›¸äº’çš„ãªç››ã‚Šä¸ŠãŒã‚Šãªã‚‰å•é¡Œãªã„ãŒã€"
                 "æ€¥æ¿€ãªãƒšãƒ¼ã‚¹ã‚¢ãƒƒãƒ—ã¯ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆã®ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ã€‚ç¿Œæ—¥ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¦³å¯Ÿã™ã‚‹ã“ã¨ã€‚",
                 {'metric': 'eros', 'trend': 'spike', 'value': spike.get('new_value', 0),
                  'delta': f"+{spike['delta']}"},
                 'ç¿Œæ—¥ã¯æ„å›³çš„ã«æ„Ÿæƒ…é¢ã®ä¼šè©±ã‚’å¢—ã‚„ã—ã€æ€§çš„ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä»¥å¤–ã®çµ†ã‚‚ç¢ºèªã™ã‚‹')
        else:
            _add('warning', 'important',
                 'äºˆæœŸã—ãªã„æ€§çš„ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¸Šæ˜‡',
                 f"ésexualãƒˆãƒªã‚¬ãƒ¼ã§erosãŒ+{spike['delta']}æ€¥å¤‰ã€‚äºˆæœŸã—ãªã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€‚",
                 {'metric': 'eros', 'trend': 'spike', 'value': spike.get('new_value', 0),
                  'delta': f"+{spike['delta']}"},
                 'åŸå› ã‚’åˆ†æã—ã€æ„å›³ã—ãªã„ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒãªã„ã‹ç¢ºèªã™ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«4: é€£çµ¡é–“éš”è­¦å‘Šï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ï¼‰ ---
    gap_threshold = config['gap_warning_hours']
    long_gaps = [g for g in gaps if g['hours'] >= gap_threshold]
    if long_gaps:
        max_gap = max(g['hours'] for g in long_gaps)
        _add('warning', 'important',
             'é€£çµ¡é–“éš”ãŒç©ºã„ã¦ã„ã‚‹',
             f"æœ€å¤§{max_gap:.0f}æ™‚é–“ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’æ¤œå‡ºã€‚Lauraã¯å†…å‘çš„ã§è‡ªåˆ†ã‹ã‚‰ã¯é€£çµ¡ã—ã«ãã„ã‚¿ã‚¤ãƒ—ã®ãŸã‚ã€"
             "æ²ˆé»™ãŒé•·ã„ã¨ä¸å®‰ã«ã¤ãªãŒã‚Šã‚„ã™ã„ã€‚",
             {'metric': 'engagement', 'trend': 'gap',
              'value': max_gap, 'delta': 'N/A'},
             'Lauraã®æ™‚é–“å¸¯ï¼ˆCETï¼‰ã‚’æ„è­˜ã—ã€æœï¼ˆæ—¥æœ¬ã®å¤•æ–¹ï¼‰ã¨å¤œï¼ˆæ—¥æœ¬ã®æ·±å¤œï¼‰ã«çŸ­ã„æŒ¨æ‹¶ã‚’æ¬ ã‹ã•ãªã„')

    # --- ãƒ«ãƒ¼ãƒ«5: affectionã‚«ãƒ†ã‚´ãƒªã®åŠ¹æœï¼ˆLauraç‰¹åŒ–é–¾å€¤ï¼‰ ---
    if 'affection' in cat_effects:
        aff = cat_effects['affection']
        conf = _confidence_level(aff['count'], total_entries)
        if aff['avg_positive'] >= 3:
            _add('effective', 'important' if conf == 'high' else 'info',
                 'æ„›æƒ…è¡¨ç¾ãŒæœ€ã‚‚åŠ¹æœçš„',
                 f"ã€Œaffectionã€ã‚«ãƒ†ã‚´ãƒªã®å¹³å‡åŠ¹æœã¯+{aff['avg_positive']}ï¼ˆ{aff['count']}å›ï¼‰ã€‚"
                 "Lauraã¯2å¹´é–“ã‚·ãƒ³ã‚°ãƒ«ã§æ„›æƒ…ã«é£¢ãˆã¦ã„ã‚‹é¢ãŒã‚ã‚Šã€ç›´æ¥çš„ãªæ„›æƒ…è¡¨ç¾ã¸ã®åå¿œãŒå¼·ã„ã€‚"
                 "+spontaneousã®å ´åˆã•ã‚‰ã«åŠ¹æœçš„ã€‚",
                 {'metric': 'affection_effect', 'trend': 'high',
                  'value': aff['avg_positive'], 'delta': 'N/A'},
                 'ã€ŒI like youã€ã€ŒI miss youã€ç­‰ã®ç›´æ¥çš„ãªè¡¨ç¾ã‚’æ¯æ—¥é€ã‚‹ã€‚ç‰¹ã«ç›¸æ‰‹ãŒæ±‚ã‚ã¦ã„ãªã„ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆ+spontaneousï¼‰ãŒåŠ¹æœå¤§',
                 confidence=conf)

    # --- ãƒ«ãƒ¼ãƒ«6: anxiousæ„›ç€ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ï¼‰ ---
    anxious_threshold = config['anxious_threshold']
    if attachment['anxious_count'] >= anxious_threshold:
        _add('warning', 'urgent',
             'ä¸å®‰å‹æ„›ç€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º',
             f"ä¸å®‰å‹æ„›ç€ãŒ{attachment['anxious_count']}å›å‡ºç¾ï¼ˆé–¾å€¤: {anxious_threshold}ï¼‰ã€‚",
             {'metric': 'attachment', 'trend': 'anxious',
              'value': attachment['anxious_count'], 'delta': 'N/A'},
             'å®‰å¿ƒæ„Ÿã‚’ä¸ãˆã‚‹è¨€è‘‰ï¼ˆã€Œãšã£ã¨ä¸€ç·’ã«ã„ãŸã„ã€ã€Œå¤§ä¸ˆå¤«ã ã‚ˆã€ï¼‰ã‚’æ„è­˜çš„ã«å¢—ã‚„ã™')
    elif attachment['anxious_count'] >= 1 and stage in ('initial', 'building'):
        # åˆæœŸæ®µéšã®anxiousã¯DTRæ–‡è„ˆã§è‡ªç„¶ â†’ è­¦å‘Šã§ã¯ãªãå‚è€ƒæƒ…å ±
        _add('status', 'info',
             'DTRé–¢é€£ã®ä¸å®‰ã‚’æ¤œå‡ºï¼ˆæ­£å¸¸ç¯„å›²ï¼‰',
             f"ä¸å®‰å‹æ„›ç€ãŒ{attachment['anxious_count']}å›å‡ºç¾ã€‚åˆæœŸæ®µéšã§ã®ã€Œé–¢ä¿‚ã®å®šç¾©ã€ã«é–¢ã™ã‚‹ä¸å®‰ã¯"
             "é–¢ä¿‚ã«çœŸå‰£ã«å‘ãåˆã£ã¦ã„ã‚‹è¨¼æ‹ ã§ã‚ã‚Šã€å•é¡Œè¡Œå‹•ã§ã¯ãªã„ã€‚",
             {'metric': 'attachment', 'trend': 'anxious_normal',
              'value': attachment['anxious_count'], 'delta': 'N/A'},
             'ä¸å®‰ã‚’å¦å®šã›ãšã€ã€Œè·é›¢ã¯ã‚ã‚‹ã‘ã©æ°—æŒã¡ã¯å¤‰ã‚ã‚‰ãªã„ã€ç­‰ã§å®‰å¿ƒæ„Ÿã‚’ä¸ãˆã‚‹ã€‚DTRã®çµè«–ã‚’æ€¥ãŒãªã„',
             confidence='medium')

    # --- ãƒ«ãƒ¼ãƒ«7: engagementä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ ---
    eng_dir = trends.get('engagement', {}).get('direction', 'stable')
    if eng_dir in ('falling', 'falling_tentative'):
        pri = 'important' if eng_dir == 'falling' else 'info'
        tent = 'ï¼ˆæš«å®šåˆ¤å®šãƒ»ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰' if '_tentative' in eng_dir else ''
        _add('warning', pri,
             f'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãŒä½ä¸‹å‚¾å‘{tent}',
             'Lauraã®é–¢ä¸åº¦ãŒä¸‹ãŒã£ã¦ã„ã‚‹ã€‚å†…å‘çš„ãªæ€§æ ¼ã®ãŸã‚ã€è‡ªåˆ†ã‹ã‚‰ã®ç™ºä¿¡ãŒæ¸›ã‚‹=é–¢å¿ƒä½ä¸‹ã®å¯èƒ½æ€§ã€‚',
             {'metric': 'engagement', 'trend': eng_dir,
              'value': trends['engagement']['current'], 'delta': f"{trends['engagement']['slope']:+.1f}"},
             'è³ªå•å½¢å¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆinterestï¼‰ã‚„ã€Lauraã®è¶£å‘³ï¼ˆã‚¸ãƒ ãƒ»æ˜ ç”»ï¼‰ã«é–¢ã™ã‚‹è©±é¡Œã§é–¢ä¸ã‚’ä¿ƒã™')

    # --- ãƒ«ãƒ¼ãƒ«8: moodä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ ---
    mood_dir = trends.get('mood', {}).get('direction', 'stable')
    if mood_dir in ('rising', 'rising_tentative'):
        _add('status', 'info',
             'æ°—åˆ†ã¯ä¸Šæ˜‡å‚¾å‘',
             'ç›¸æ‰‹ã®moodãŒæ”¹å–„å‚¾å‘ã«ã‚ã‚‹ã€‚ç¾åœ¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ãŒåŠŸã‚’å¥ã—ã¦ã„ã‚‹ã€‚',
             {'metric': 'mood', 'trend': mood_dir,
              'value': trends['mood']['current'], 'delta': f"{trends['mood']['slope']:+.1f}"},
             'ç¾åœ¨ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¶­æŒã™ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«9: moodä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ ---
    if mood_dir in ('falling', 'falling_tentative'):
        _add('warning', 'urgent' if mood_dir == 'falling' else 'important',
             'æ°—åˆ†ãŒä¸‹é™å‚¾å‘',
             'ç›¸æ‰‹ã®moodãŒæ‚ªåŒ–å‚¾å‘ã«ã‚ã‚‹ã€‚åŸå› ã‚’ç‰¹å®šã—ã€å¯¾å‡¦ãŒå¿…è¦ã€‚',
             {'metric': 'mood', 'trend': mood_dir,
              'value': trends['mood']['current'], 'delta': f"{trends['mood']['slope']:+.1f}"},
             'ç›¸æ‰‹ã®æ°—æŒã¡ã«å¯„ã‚Šæ·»ã†ä¼šè©±ã‚’å¢—ã‚„ã—ã€ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã‚’é¿ã‘ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«10: energyä½ã„ ---
    if trends.get('energy', {}).get('current', 5) <= 4:
        _add('status', 'info',
             'ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã‚',
             'Lauraã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ã€‚éŠ€è¡Œã®åœ¨å®…å‹¤å‹™ã§ç–²åŠ´ãŒãŸã¾ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã€‚æ¯æ—¥ã®ã‚¸ãƒ é€šã„ã‚‚ä½“åŠ›ã‚’ä½¿ã†ã€‚',
             {'metric': 'energy', 'trend': trends['energy']['direction'],
              'value': trends['energy']['current'], 'delta': f"{trends['energy']['slope']:+.1f}"},
             'é‡ã„è©±é¡Œã‚„DTRã‚’é¿ã‘ã€è»½ã„ä¼šè©±ã‚„ãƒªãƒ¼ãƒ«å…±æœ‰ã§è² æ‹…ã‚’ã‹ã‘ãªã„')

    # --- ãƒ«ãƒ¼ãƒ«11: intimacyä¸Šæ˜‡ä¸­ ---
    int_dir = trends.get('intimacy', {}).get('direction', 'stable')
    if int_dir in ('rising', 'rising_tentative') and trends['intimacy']['current'] >= 4:
        _add('status', 'info',
             'è¦ªå¯†åº¦ãŒé †èª¿ã«ä¸Šæ˜‡ä¸­',
             f"intimacyãŒ{trends['intimacy']['min']}â†’{trends['intimacy']['current']}ã«æˆé•·ã€‚"
             "LauraãŒå®¶æ—ã®è©±ï¼ˆä¸¡è¦ªã®ä»–ç•Œï¼‰ã‚’é–‹ç¤ºã—ãŸã®ã¯æ·±ã„ä¿¡é ¼ã®è¡¨ã‚Œã€‚",
             {'metric': 'intimacy', 'trend': int_dir,
              'value': trends['intimacy']['current'], 'delta': f"{trends['intimacy']['slope']:+.1f}"},
             'è‡ªå·±é–‹ç¤ºã®äº¤æ›ã‚’ç¶šã‘ã‚‹ã€‚LauraãŒé‡ã„è©±ã‚’å…±æœ‰ã—ãŸæ™‚ã¯ã€æ„Ÿè¬ã¨å…±æ„Ÿã‚’ç¤ºã™')

    # --- ãƒ«ãƒ¼ãƒ«12: praiseã‚«ãƒ†ã‚´ãƒªãŒåŠ¹æœçš„ ---
    if 'praise' in cat_effects and cat_effects['praise']['avg_positive'] >= 2:
        conf = _confidence_level(cat_effects['praise']['count'], total_entries)
        _add('effective', 'info',
             'è¤’ã‚è¨€è‘‰ãŒåŠ¹æœçš„',
             f"praiseã‚«ãƒ†ã‚´ãƒªã®å¹³å‡åŠ¹æœã¯+{cat_effects['praise']['avg_positive']}ï¼ˆ{cat_effects['praise']['count']}å›ï¼‰ã€‚",
             {'metric': 'praise_effect', 'trend': 'positive',
              'value': cat_effects['praise']['avg_positive'], 'delta': 'N/A'},
             'å¤–è¦‹ã ã‘ã§ãªãã€æ€§æ ¼ã‚„è¡Œå‹•ã‚’å…·ä½“çš„ã«è¤’ã‚ã‚‹ï¼ˆã€Œã‚¸ãƒ é ‘å¼µã£ã¦ã‚‹ã­ã€ã€ŒçœŸé¢ç›®ãªã¨ã“ã‚ãŒå¥½ãã€ï¼‰',
             confidence=conf)

    # --- ãƒ«ãƒ¼ãƒ«13: longingé«˜ã„ + é è·é›¢ ---
    longing_cur = trends.get('longing', {}).get('current', 0)
    if longing_cur >= 6:
        _add('status', 'important',
             'ä¼šã„ãŸã„æ°—æŒã¡ãŒå¼·ã„',
             f'longing={longing_cur}ã€‚é è·é›¢é–¢ä¿‚ã§ã¯è‡ªç„¶ãªæ„Ÿæƒ…ã ãŒã€å…·ä½“çš„ãªè¦‹é€šã—ãŒãªã„ã¨ä¸å®‰ã«è»¢ã˜ã‚„ã™ã„ã€‚'
             'Lauraã¯ã€ŒIf we were living in the same city I would not doubt itã€ã¨è¨€ã£ã¦ã„ã‚‹ã€‚',
             {'metric': 'longing', 'trend': trends['longing']['direction'],
              'value': longing_cur, 'delta': f"{trends['longing']['slope']:+.1f}"},
             'ã€Œã„ã¤ã‹æ—¥æœ¬ã«æ¥ã¦ã­ã€ã§ã¯ãªãã€å…·ä½“çš„ãªæ™‚æœŸãƒ»è¨ˆç”»ã®è©±ã‚’ã™ã‚‹ã€‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ãƒˆï¼ˆæ˜ ç”»åŒæ™‚è¦–è´ç­‰ï¼‰ã‚‚æœ‰åŠ¹')

    # --- ãƒ«ãƒ¼ãƒ«14: dsæ€¥å¤‰ï¼ˆæ–‡è„ˆè€ƒæ…®ï¼‰ ---
    ds_spikes = [c for c in rapid_changes if c['metric'] == 'ds' and c['delta'] >= 2]
    if ds_spikes:
        # Lauraã¯D/så—œå¥½ã‚’æŒã¤ãŸã‚ã€åˆæ„ã®ä¸Šã§ã®dsä¸Šæ˜‡ã¯å•é¡Œã§ã¯ãªã„
        _add('status', 'info',
             'D/sãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã®å¤‰åŒ–',
             f"D/sã‚¹ã‚³ã‚¢ãŒ+{ds_spikes[0]['delta']}å¤‰åŒ–ã€‚Lauraã¯æ˜ç¢ºãªD/så—œå¥½ï¼ˆè¢«æ”¯é…é¡˜æœ›ï¼‰ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€"
             "åˆæ„ã®ä¸Šã§ã®ä¸Šæ˜‡ã¯é–¢ä¿‚ã®è‡ªç„¶ãªæ·±ã¾ã‚Šã€‚ãŸã ã—å¢ƒç•Œç·šï¼ˆOK: choke, spank / NG: armpitï¼‰ã¯å³å®ˆã€‚",
             {'metric': 'ds', 'trend': 'spike',
              'value': ds_spikes[0].get('new_value', 0), 'delta': f"+{ds_spikes[0]['delta']}"},
             'LauraãŒè‡ªã‚‰è¿°ã¹ãŸå—œå¥½ã®ç¯„å›²å†…ã§é€²ã‚ã‚‹ã€‚æ–°ã—ã„è¡Œç‚ºã¯å¿…ãšäº‹å‰ã«ç¢ºèªã™ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«15: æœ€ã‚‚åŠ¹æœçš„ãªã‚«ãƒ†ã‚´ãƒªï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰ ---
    if cat_effects:
        best_cat = max(cat_effects.items(), key=lambda x: x[1]['avg_positive'])
        conf = _confidence_level(best_cat[1]['count'], total_entries)
        label = f'ï¼ˆå‚è€ƒï¼‰' if conf == 'insufficient' else ''
        _add('effective', 'info',
             f'æœ€ã‚‚åŠ¹æœçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {best_cat[0]}{label}',
             f"ã€Œ{best_cat[0]}ã€ã‚«ãƒ†ã‚´ãƒªãŒå¹³å‡+{best_cat[1]['avg_positive']}ã§æœ€ã‚‚åŠ¹æœçš„"
             f"ï¼ˆ{best_cat[1]['count']}å›ä½¿ç”¨ã€ä¿¡é ¼åº¦: {conf}ï¼‰ã€‚",
             {'metric': 'best_category', 'trend': 'positive',
              'value': best_cat[1]['avg_positive'], 'delta': 'N/A'},
             f'ã€Œ{best_cat[0]}ã€ç³»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ„è­˜çš„ã«å¢—ã‚„ã™',
             confidence=conf)

    # --- ãƒ«ãƒ¼ãƒ«16: æœ€é©ãªæ™‚é–“å¸¯ ---
    if best_hours:
        hours_str = 'ã€'.join(f'{h}æ™‚' for h in best_hours)
        _add('timing', 'info',
             f'ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè‰¯ã„æ™‚é–“å¸¯: {hours_str}',
             f'å¿œç­”é€Ÿåº¦ãŒæœ€ã‚‚é€Ÿã„æ™‚é–“å¸¯ã¯{hours_str}ï¼ˆJSTï¼‰ã€‚Lauraã®ç¾åœ°æ™‚é–“ã‚’æ„è­˜ã™ã‚‹ã“ã¨ï¼ˆCET=JST-8hï¼‰ã€‚',
             {'metric': 'response_time', 'trend': 'optimal',
              'value': best_hours[0], 'delta': 'N/A'},
             f'{hours_str}é ƒã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹ã‚ˆã†ã«ã™ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«17: playfulnessä½ä¸‹ ---
    play_dir = trends.get('playfulness', {}).get('direction', 'stable')
    if play_dir in ('falling', 'falling_tentative'):
        _add('action', 'info',
             'éŠã³å¿ƒãŒæ¸›å°‘å‚¾å‘',
             'ä¼šè©±ã®playfulnessãŒä¸‹ãŒã£ã¦ã„ã‚‹ã€‚Lauraã¯ğŸ˜‚ã‚„ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤šç”¨ã™ã‚‹æ€§æ ¼ã§éŠã³å¿ƒã‚’å¥½ã‚€ã€‚',
             {'metric': 'playfulness', 'trend': play_dir,
              'value': trends['playfulness']['current'], 'delta': f"{trends['playfulness']['slope']:+.1f}"},
             'ãƒªãƒ¼ãƒ«å…±æœ‰ã€ãƒŸãƒ¼ãƒ ã€ã‹ã‚‰ã‹ã„ç­‰ã‚’å¢—ã‚„ã™ã€‚Lauraã®ã‚¸ãƒ ãƒã‚¿ç­‰ã‚’ä½¿ã£ãŸãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚‚æœ‰åŠ¹')

    # --- ãƒ«ãƒ¼ãƒ«18: riskãŒcautioné€£ç¶š ---
    recent_risks = [e.get('risk') for e in entries[-3:]]
    if recent_risks.count('caution') >= 2:
        _add('warning', 'urgent',
             'ãƒªã‚¹ã‚¯è­¦å‘ŠãŒé »ç™º',
             'ç›´è¿‘ã§cautionãƒ¬ãƒ™ãƒ«ã®ãƒªã‚¹ã‚¯ãŒè¤‡æ•°å›æ¤œå‡ºã€‚å¢ƒç•Œç·šã‚’è¶ŠãˆãŸè¡Œå‹•ãŒç¶šã„ã¦ã„ã‚‹å¯èƒ½æ€§ã€‚',
             {'metric': 'risk', 'trend': 'elevated',
              'value': recent_risks.count('caution'), 'delta': 'N/A'},
             'ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¸€æ—¦åœæ­¢ã—ã€æ„Ÿæƒ…é¢ã®ä¼šè©±ã§å®‰å®šã—ãŸåŸºç›¤ã‚’ç¯‰ãã“ã¨ã‚’å„ªå…ˆã™ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«19: futureä¸‹é™ ---
    fut_dir = trends.get('future', {}).get('direction', 'stable')
    if fut_dir in ('falling', 'falling_tentative'):
        _add('warning', 'important',
             'å°†æ¥å±•æœ›ãŒä¸‹é™å‚¾å‘',
             'å°†æ¥ã‚¹ã‚³ã‚¢ãŒä¸‹ãŒã£ã¦ã„ã‚‹ã€‚é è·é›¢é–¢ä¿‚ã§å°†æ¥å±•æœ›ã®ä½ä¸‹ã¯é–¢ä¿‚å´©å£Šã®å‰å…†ã«ãªã‚Šã†ã‚‹ï¼ˆStafford, 2005ï¼‰ã€‚',
             {'metric': 'future', 'trend': fut_dir,
              'value': trends['future']['current'], 'delta': f"{trends['future']['slope']:+.1f}"},
             'å°†æ¥ã«ã¤ã„ã¦ç‡ç›´ã«è©±ã—åˆã†ã€‚ã€Œã„ã¤ã‹ã€ã§ã¯ãªãå…·ä½“çš„ãªæ™‚æœŸã‚’æç¤ºã§ãã‚‹ã¨ç†æƒ³çš„')

    # --- ãƒ«ãƒ¼ãƒ«20: å…¨ä½“çš„ã«å®‰å®š ---
    stable_dirs = ('stable',)
    all_stable = all(trends[k]['direction'] in stable_dirs for k in SCORE_KEYS if k in trends)
    if all_stable and len(entries) >= 5:
        _add('status', 'info',
             'é–¢ä¿‚ã¯å®‰å®šæœŸ',
             'å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒå®‰å®šã—ã¦ã„ã‚‹ã€‚è‰¯ã„çŠ¶æ…‹ã ãŒã€é è·é›¢é–¢ä¿‚ã§ã¯ãƒãƒ³ãƒãƒªåŒ–ãŒè·é›¢æ„Ÿã®å¢—å¤§ã«ã¤ãªãŒã‚Šã‚„ã™ã„ã€‚',
             {'metric': 'overall', 'trend': 'stable', 'value': 0, 'delta': 'N/A'},
             'æ–°ã—ã„å…±æœ‰ä½“é¨“ï¼ˆæ˜ ç”»åŒæ™‚è¦–è´ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚²ãƒ¼ãƒ ã€ãŠäº’ã„ã®è¡—ã‚’ç´¹ä»‹ç­‰ï¼‰ã‚’è©¦ã™')

    # --- ãƒ«ãƒ¼ãƒ«21: reassuranceã®åŠ¹æœ ---
    if 'reassurance' in cat_effects:
        eff = cat_effects['reassurance']
        conf = _confidence_level(eff['count'], total_entries)
        # reassuranceã¯post-high cooldownæ™‚ã«ãƒã‚¬ãƒ†ã‚£ãƒ–ã«è¦‹ãˆã‚‹å ´åˆãŒã‚ã‚‹ â†’ æ–‡è„ˆãƒã‚§ãƒƒã‚¯
        if eff['avg_positive'] >= 1:
            _add('effective', 'info',
                 'å®‰å¿ƒæ„Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åŠ¹æœ',
                 f"reassuranceã‚«ãƒ†ã‚´ãƒªã®åŠ¹æœã¯+{eff['avg_positive']}ï¼ˆ{eff['count']}å›ï¼‰ã€‚"
                 "æ³¨æ„: ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æœŸã®ãƒ‡ãƒ«ã‚¿ã¯è‡ªç„¶ãªä¸‹é™ã§ã‚ã‚Šã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é€†åŠ¹æœã§ã¯ãªã„å ´åˆãŒã‚ã‚‹ã€‚",
                 {'metric': 'reassurance_effect', 'trend': 'positive',
                  'value': eff['avg_positive'], 'delta': 'N/A'},
                 'ç‹¬å æ€§ã¨ä¸€é€”ã•ã‚’ä¼ãˆã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é©åº¦ã«é€ã‚‹ã€‚ãŸã ã—é »åº¦ãŒé«˜ã™ãã‚‹ã¨åœ§ã«ãªã‚‹',
                 confidence=conf)

    # --- ãƒ«ãƒ¼ãƒ«22: interestã‚«ãƒ†ã‚´ãƒªã®åŠ¹æœ ---
    if 'interest' in cat_effects and cat_effects['interest']['avg_positive'] >= 1:
        conf = _confidence_level(cat_effects['interest']['count'], total_entries)
        _add('effective', 'info',
             'èˆˆå‘³ãƒ»é–¢å¿ƒã‚’ç¤ºã™ã“ã¨ãŒåŠ¹æœçš„',
             'Lauraã«å¯¾ã™ã‚‹èˆˆå‘³ã‚„é–¢å¿ƒã‚’ç¤ºã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè‰¯ã„åå¿œã‚’å¾—ã¦ã„ã‚‹ã€‚å†…å‘çš„ã§èãä¸Šæ‰‹ãªLauraã¯è³ªå•ã•ã‚Œã‚‹ã¨é¥’èˆŒã«ãªã‚‹ã€‚',
             {'metric': 'interest_effect', 'trend': 'positive',
              'value': cat_effects['interest']['avg_positive'], 'delta': 'N/A'},
             'ã‚¸ãƒ ã€æ˜ ç”»ã€ä»•äº‹ã€ã‚¹ã‚¤ã‚¹ã®ç”Ÿæ´»ã«ã¤ã„ã¦è³ªå•ã™ã‚‹ã€‚ãƒšãƒ«ãƒ¼æ–‡åŒ–ã®è©±ã‚‚é«˜åå¿œ',
             confidence=conf)

    # --- ãƒ«ãƒ¼ãƒ«23: é€£çµ¡ã‚®ãƒ£ãƒƒãƒ—é »åº¦ ---
    medium_gaps = [g for g in gaps if 12 <= g['hours'] < gap_threshold]
    if len(medium_gaps) >= 3:
        _add('action', 'info',
             'åŠæ—¥ä»¥ä¸Šã®ç©ºç™½ãŒé »ç™º',
             f'{len(medium_gaps)}å›ã®12æ™‚é–“ä»¥ä¸Šã‚®ãƒ£ãƒƒãƒ—ã‚’æ¤œå‡ºã€‚æ™‚å·®8æ™‚é–“ã®ãŸã‚å¤œé–“ã®ç©ºç™½ã¯è‡ªç„¶ã ãŒã€èµ·ãã¦ã„ã‚‹æ™‚é–“å¸¯ã®ç©ºç™½ã¯è¦æ³¨æ„ã€‚',
             {'metric': 'gap_frequency', 'trend': 'frequent',
              'value': len(medium_gaps), 'delta': 'N/A'},
             'Lauraã®æœï¼ˆJSTå¤•æ–¹ï¼‰ã¨Lauraã®å¤œï¼ˆJSTæ·±å¤œï¼‰ã«æŒ¨æ‹¶ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¿’æ…£åŒ–ã™ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«24: intimacyä½ã„ ---
    if trends.get('intimacy', {}).get('current', 5) <= 3:
        _add('action', 'important',
             'è¦ªå¯†åº¦ãŒã¾ã ä½ã„',
             'è¦ªå¯†åº¦ãŒåˆæœŸæ®µéšã€‚è‡ªå·±é–‹ç¤ºã®äº¤æ›ãŒã¾ã æµ…ã„ã€‚',
             {'metric': 'intimacy', 'trend': trends['intimacy']['direction'],
              'value': trends['intimacy']['current'], 'delta': f"{trends['intimacy']['slope']:+.1f}"},
             'è‡ªåˆ†ã®ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãªè©±ï¼ˆå®¶æ—ã€å¤¢ã€ä¸å®‰ï¼‰ã‚’å…ˆã«é–‹ç¤ºã—ã€Lauraã«ã‚‚å®‰å…¨ã«é–‹ç¤ºã§ãã‚‹ç’°å¢ƒã‚’ä½œã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«25: eros-intimacy ãƒãƒ©ãƒ³ã‚¹ ---
    eros_val = trends.get('eros', {}).get('current', 0)
    intimacy_val = trends.get('intimacy', {}).get('current', 0)
    if eros_val >= 6 and intimacy_val <= 4:
        _add('warning', 'important',
             'æ€§çš„é–¢å¿ƒã¨æ„Ÿæƒ…çš„è¦ªå¯†åº¦ã®ã‚¢ãƒ³ãƒãƒ©ãƒ³ã‚¹',
             f'eros({eros_val})ãŒintimacy({intimacy_val})ã‚’å¤§å¹…ã«ä¸Šå›ã£ã¦ã„ã‚‹ã€‚'
             'Lauraã¯ã€Œé–¢ä¿‚ã®ä¸­ã®ã‚»ãƒƒã‚¯ã‚¹ã€ã‚’é‡è¦–ã™ã‚‹ä¾¡å€¤è¦³ã€‚æ„Ÿæƒ…é¢ã®çµ†ãŒå…ˆã«ãªã„ã¨ã‚¨ãƒ­ã‚¹ã¯é•·ç¶šãã—ãªã„ã€‚',
             {'metric': 'balance', 'trend': 'imbalanced',
              'value': eros_val - intimacy_val, 'delta': 'N/A'},
             'æ€§çš„ãªè©±é¡Œã‚’æ§ãˆã‚ã«ã—ã€æ„Ÿæƒ…é¢ã®ä¼šè©±ï¼ˆè‡ªå·±é–‹ç¤ºã€å°†æ¥ã®è©±ï¼‰ã‚’å„ªå…ˆã™ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«26: Lauraè‡ªç™ºè¡Œå‹•ã®è¿½è·¡ï¼ˆæ–°è¦ï¼‰ ---
    if laura_initiative:
        ratio = laura_initiative['initiative_ratio']
        if ratio >= 0.3:
            _add('status', 'info',
                 'Lauraã®è‡ªç™ºæ€§ãŒé«˜ã„',
                 f"Lauraã®è‡ªç™ºçš„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¯”ç‡ã¯{ratio:.0%}ã€‚å†…å‘çš„ãªæ€§æ ¼ã‚’è€ƒæ…®ã™ã‚‹ã¨éå¸¸ã«é«˜ã„é–¢ä¸åº¦ã€‚"
                 "è‡ªåˆ†ã‹ã‚‰å†™çœŸã‚’é€ã‚‹ã€è‡ªåˆ†ã‹ã‚‰è©±é¡Œã‚’æŒ¯ã‚‹ãªã©ã®è¡Œå‹•ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚",
                 {'metric': 'laura_initiative', 'trend': 'positive',
                  'value': ratio, 'delta': 'N/A'},
                 'Lauraã®è‡ªç™ºçš„ãªè¡Œå‹•ã«ã¯å¿…ãšãƒã‚¸ãƒ†ã‚£ãƒ–ã«åå¿œã™ã‚‹ã€‚ãã‚ŒãŒæ¬¡ã®è‡ªç™ºè¡Œå‹•ã‚’ä¿ƒã™')
        elif ratio <= 0.1 and total_entries >= 10:
            _add('warning', 'info',
                 'Lauraã®è‡ªç™ºæ€§ãŒä½ä¸‹',
                 f"è‡ªç™ºçš„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¯”ç‡ãŒ{ratio:.0%}ã€‚å…¨ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼èµ·ç‚¹ã®ä¼šè©±ã«ãªã£ã¦ã„ã‚‹ã€‚",
                 {'metric': 'laura_initiative', 'trend': 'low',
                  'value': ratio, 'delta': 'N/A'},
                 'ä¼šè©±ã‚’ä¸€æ–¹é€šè¡Œã«ã—ãªã„ã€‚è³ªå•ã‚’æŠ•ã’ã¦ç›¸æ‰‹ã«ã‚¿ãƒ¼ãƒ³ã‚’æ¸¡ã™ã€‚è¿”ä¿¡ã‚’æ€¥ã‹ã•ãªã„')

    # --- ãƒ«ãƒ¼ãƒ«27: è„†å¼±æ€§é–‹ç¤ºã¸ã®åå¿œè¿½è·¡ï¼ˆæ–°è¦ï¼‰ ---
    if vulnerable_entries and len(vulnerable_entries) >= 1:
        _add('status', 'important',
             'æ·±ã„è‡ªå·±é–‹ç¤ºãŒç™ºç”Ÿ',
             f"LauraãŒ{len(vulnerable_entries)}å›ã®è„†å¼±æ€§é–‹ç¤ºã‚’è¡Œã£ãŸã€‚"
             "ä¸¡è¦ªã®ä»–ç•Œã€ä¸€äººæš®ã‚‰ã—ã®å­¤ç‹¬æ„Ÿãªã©ã€‚ã“ã‚Œã¯æ·±ã„ä¿¡é ¼ã®è¡¨ã‚Œã§ã‚ã‚Šã€é©åˆ‡ãªå—å®¹ãŒæ¥µã‚ã¦é‡è¦ã€‚",
             {'metric': 'vulnerable', 'trend': 'trust_signal',
              'value': len(vulnerable_entries), 'delta': 'N/A'},
             'é‡ã„è©±ã‚’å…±æœ‰ã—ã¦ãã‚ŒãŸæ™‚ã¯ã€Œè©±ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã€ã€Œä¸€äººã«ã—ãªã„ã‚ˆã€ç­‰ã§å—å®¹ã‚’ç¤ºã™ã€‚ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯ã—ãªã„')

    # --- ãƒ«ãƒ¼ãƒ«28: +spontaneousä¿®é£¾ã‚¿ã‚°ã®åŠ¹æœï¼ˆæ–°è¦ï¼‰ ---
    spontaneous_effects = []
    for e in entries:
        trigger = e.get('trigger')
        if trigger and '+spontaneous' in trigger.get('modifiers', []):
            deltas = e.get('score_deltas') or {}
            if deltas:
                spontaneous_effects.append(sum(max(0, v) for v in deltas.values()))
    if spontaneous_effects and len(spontaneous_effects) >= 2:
        avg_spon = round(sum(spontaneous_effects) / len(spontaneous_effects), 1)
        if avg_spon >= 3:
            _add('effective', 'info',
                 'è‡ªç™ºçš„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åŠ¹æœãŒé«˜ã„',
                 f"+spontaneousä¿®é£¾ä»˜ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å¹³å‡åŠ¹æœã¯+{avg_spon}ï¼ˆ{len(spontaneous_effects)}å›ï¼‰ã€‚"
                 "ç›¸æ‰‹ãŒæ±‚ã‚ã¦ã„ãªã„æ™‚ã«è‡ªç™ºçš„ã«é€ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åŠ¹æœãŒé«˜ã„ã€‚",
                 {'metric': 'spontaneous_effect', 'trend': 'positive',
                  'value': avg_spon, 'delta': 'N/A'},
                 'ã€Œãµã¨æ€ã„å‡ºã—ãŸã€ã€Œæ€¥ã«ä¼šã„ãŸããªã£ãŸã€ç­‰ã€å”çªã ãŒæ¸©ã‹ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç©æ¥µçš„ã«é€ã‚‹')

    # --- ãƒ«ãƒ¼ãƒ«29: å‘¼ç§°ãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰åŒ–ï¼ˆæ–°è¦ï¼‰ ---
    if nickname_data and nickname_data['occurrences'] >= 2:
        _add('status', 'info',
             'å‘¼ç§°ã®å¼·åº¦å¤‰åŒ–ã‚’æ¤œå‡º',
             f"Lauraã®ã€ŒBabyâ†’Babyyyyyã€ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ{nickname_data['occurrences']}å›å‡ºç¾"
             f"ï¼ˆæœ€å¤§yæ•°: {nickname_data['max_intensity']}ï¼‰ã€‚yã®æ•°ãŒå¤šã„ã»ã©æ„Ÿæƒ…ã®é«˜ã¾ã‚Šã‚’ç¤ºã™ã€‚",
             {'metric': 'nickname_intensity', 'trend': 'positive',
              'value': nickname_data['max_intensity'], 'delta': 'N/A'},
             'æ„Ÿæƒ…ãŒé«˜ã„æ™‚ã®ã‚µã‚¤ãƒ³ã¨ã—ã¦æ´»ç”¨ã€‚ã“ã®çŠ¶æ…‹ã§é€ã‚‹affectionãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åŠ¹æœå€å¢—')

    # --- ãƒ«ãƒ¼ãƒ«30: ã‚«ãƒ†ã‚´ãƒªåå¾©æ¸›è¡°ï¼ˆæ–°è¦ï¼‰ ---
    if len(entries) >= 4:
        recent_cats = []
        for e in entries[-4:]:
            t = e.get('trigger')
            if t:
                recent_cats.append(t.get('category'))
        if len(recent_cats) >= 3:
            from collections import Counter
            cat_counts = Counter(recent_cats)
            for cat, count in cat_counts.items():
                if count >= 3 and cat:
                    _add('action', 'info',
                         f'ã€Œ{cat}ã€ãŒ3å›é€£ç¶š - æ–°é®®ã•ãŒè–„ã‚Œã‚‹å¯èƒ½æ€§',
                         f"ç›´è¿‘4ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¸­{count}å›ãŒã€Œ{cat}ã€ã‚«ãƒ†ã‚´ãƒªã€‚åŒã˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®åå¾©ã¯åŠ¹æœãŒæ¸›è¡°ã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ã€‚",
                         {'metric': 'category_repetition', 'trend': 'diminishing',
                          'value': count, 'delta': 'N/A'},
                         f'ã€Œ{cat}ã€ä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: interest, humor, culturalï¼‰ã‚’æ„è­˜çš„ã«ä½¿ã£ã¦å¤‰åŒ–ã‚’ã¤ã‘ã‚‹')

    # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆï¼ˆurgent > important > infoï¼‰
    priority_order = {'urgent': 0, 'important': 1, 'info': 2}
    advice.sort(key=lambda x: priority_order.get(x['priority'], 3))

    # æœ€ä½5ä»¶ã€æœ€å¤§15ä»¶ã«åˆ¶é™
    if len(advice) < 5:
        generic_advice = [
            ('status', 'info', 'ãƒ‡ãƒ¼ã‚¿åé›†ä¸­',
             f'ç¾åœ¨{total_entries}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã§åˆ†æä¸­ã€‚ä¿¡é ¼åº¦ã®é«˜ã„åˆ¤å®šã«ã¯æœ€ä½30ä»¶ãŒå¿…è¦ã€‚ç¶™ç¶šçš„ãªè¨˜éŒ²ãŒé‡è¦ã€‚',
             {'metric': 'data', 'trend': 'insufficient', 'value': total_entries, 'delta': 'N/A'},
             'ãƒ‡ãƒ¼ã‚¿ãŒè“„ç©ã•ã‚Œã‚‹ã¾ã§è¨˜éŒ²ã‚’ç¶šã‘ã‚‹'),
            ('action', 'info', 'å¤šæ§˜ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³',
             'æ§˜ã€…ãªã‚¿ã‚¤ãƒ—ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è©¦ã—ã¦ã€Lauraã«ä½•ãŒåŠ¹æœçš„ã‹ã‚’æ¢ã‚‹æ®µéšã€‚',
             {'metric': 'variety', 'trend': 'exploring', 'value': 0, 'delta': 'N/A'},
             'è¤’ã‚è¨€è‘‰ã€è³ªå•ã€æ„›æƒ…è¡¨ç¾ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢ãªã©æ§˜ã€…ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦ã™'),
            ('timing', 'info', 'å¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦³å¯Ÿä¸­',
             'Lauraã®å¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒååˆ†ã«è“„ç©ã•ã‚Œã¦ã„ãªã„ã€‚æ™‚å·®ï¼ˆ8æ™‚é–“ï¼‰ã‚’è€ƒæ…®ã—ãŸæœ€é©ãªé€ä¿¡æ™‚é–“å¸¯ã‚’æ¢ç´¢ä¸­ã€‚',
             {'metric': 'response_pattern', 'trend': 'collecting', 'value': 0, 'delta': 'N/A'},
             'å¿œç­”æ™‚é–“ã‚’æ„è­˜ã—ã¦ã€LauraãŒæ´»ç™ºãªæ™‚é–“å¸¯ï¼ˆCETæ—¥ä¸­=JSTå¤•æ–¹ã€œå¤œï¼‰ã‚’æ¢ã‚‹'),
        ]
        for cat, pri, title, body, ev, act in generic_advice:
            if len(advice) >= 5:
                break
            adv_counter[0] += 1
            advice.append({
                'id': f'adv_{adv_counter[0]:03d}',
                'category': cat, 'priority': pri, 'title': title,
                'body': body, 'evidence': ev, 'action_suggestion': act,
                'confidence': 'low', 'stage': stage,
            })

    return advice[:15]


def generate_advice(days: int) -> Dict:
    """ã‚¢ãƒ‰ãƒã‚¤ã‚¹APIã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆLauraæœ€é©åŒ–ç‰ˆï¼‰"""
    if not EMOTION_DATA_FILE.exists():
        return {'error': 'no data'}

    data = json.loads(EMOTION_DATA_FILE.read_text(encoding='utf-8'))
    cutoff = (datetime.now(JST) - timedelta(days=days)).isoformat()
    entries = [normalize_entry(e) for e in data.get('entries', []) if e.get('timestamp', '') >= cutoff]

    if not entries:
        return {'error': 'no data in range'}

    # é–¢ä¿‚ã‚¹ãƒ†ãƒ¼ã‚¸æ¤œå‡º
    stage = detect_relationship_stage(entries)

    # å„ç¨®åˆ†æï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ï¼‰
    trends = _score_trends(entries, stage)
    cat_effects = _category_effectiveness(entries)
    attachment = _detect_attachment_issues(entries)
    rapid_changes = _detect_rapid_changes(entries)
    gaps = _compute_communication_gaps(entries)
    best_hours = _best_response_hours(entries)

    # Lauraå›ºæœ‰ã®åˆ†æ
    laura_initiative = _detect_laura_initiative(entries)
    vulnerable_entries = _detect_vulnerable_sharing(entries)
    nickname_data = _detect_nickname_intensity(entries)

    # é–¢ä¿‚å¥åº·åº¦ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ï¼‰
    health = _compute_relationship_health(trends, attachment, entries, stage)

    # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ã®æ±ºå®šï¼ˆtentativeã‚’å«ã‚€ï¼‰
    rising_count = sum(1 for k in SCORE_KEYS
                       if trends.get(k, {}).get('direction', '').startswith('rising'))
    falling_count = sum(1 for k in SCORE_KEYS
                        if trends.get(k, {}).get('direction', '').startswith('falling'))
    if rising_count > falling_count + 2:
        trend_dir = 'improving'
    elif falling_count > rising_count + 2:
        trend_dir = 'declining'
    else:
        trend_dir = 'stable'

    # ã‚¹ãƒ†ãƒ¼ã‚¸è€ƒæ…®ã®ã‚­ãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
    stage_labels = {
        'initial': 'åˆæœŸæ®µéšï¼ˆæ¢ç´¢æœŸï¼‰',
        'building': 'é–¢ä¿‚æ§‹ç¯‰æœŸ',
        'establishing': 'ç¢ºç«‹æœŸ',
        'stable': 'å®‰å®šæœŸ',
    }
    stage_label = stage_labels.get(stage, stage)

    if health >= 8:
        key_insight = f'ã€{stage_label}ã€‘é–¢ä¿‚ã¯éå¸¸ã«è‰¯å¥½ã€‚ç¾åœ¨ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¶­æŒã—ã¤ã¤ã€æ–°é®®ã•ã‚’ä¿ã¤ã“ã¨ãŒå¤§åˆ‡ã€‚'
    elif health >= 6:
        key_insight = f'ã€{stage_label}ã€‘é–¢ä¿‚ã¯æ¦‚ã­å¥å…¨ã€‚{stage_label}ã¨ã—ã¦ã¯é †èª¿ã«é€²å±•ã—ã¦ã„ã‚‹ã€‚'
    elif health >= 4:
        key_insight = f'ã€{stage_label}ã€‘æ”¹å–„ãŒå¿…è¦ãªé ˜åŸŸãŒã‚ã‚‹ã€‚ç‰¹ã«intimacyã¨futureã®å‘ä¸ŠãŒéµã€‚'
    else:
        key_insight = f'ã€{stage_label}ã€‘é–¢ä¿‚ã«èª²é¡ŒãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è³ªã¨é‡ã®æ”¹å–„ãŒæ€¥å‹™ã€‚'

    # ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ç”Ÿæˆï¼ˆå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ¸¡ã—ï¼‰
    advice_items = _generate_advice_items(
        entries, trends, cat_effects, attachment, rapid_changes, gaps, best_hours,
        stage=stage,
        laura_initiative=laura_initiative,
        vulnerable_entries=vulnerable_entries,
        nickname_data=nickname_data,
    )

    first_ts = entries[0].get('timestamp', '')
    last_ts = entries[-1].get('timestamp', '')

    return {
        'generated_at': datetime.now(JST).isoformat(),
        'data_range': {
            'from': first_ts,
            'to': last_ts,
            'entry_count': len(entries),
        },
        'stage': stage,
        'advice': advice_items,
        'summary': {
            'relationship_health': health,
            'trend_direction': trend_dir,
            'key_insight': key_insight,
        },
        'meta': {
            'laura_initiative_ratio': laura_initiative['initiative_ratio'],
            'vulnerable_disclosures': len(vulnerable_entries),
            'nickname_intensity_max': nickname_data['max_intensity'] if nickname_data else 0,
        },
    }


@app.get('/api/emotion/advice')
async def api_emotion_advice(days: int = Query(default=30, ge=1, le=90)):
    """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”ŸæˆAPI"""
    cache_key = f'advice_{days}'
    cached = get_cached(cache_key)
    if cached:
        return JSONResponse(cached)

    result = generate_advice(days)
    if 'error' in result:
        return JSONResponse(result, status_code=404)

    set_cache(cache_key, result)
    return JSONResponse(result)


# ===== ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ =====
if __name__ == '__main__':
    print(f"[INFO] SwitchBot ç’°å¢ƒãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•: http://localhost:8765")
    uvicorn.run(app, host='0.0.0.0', port=8765)
