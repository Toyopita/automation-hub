#!/usr/bin/env python3
"""
æ¯æ™‚Idiomã‚µãƒ¼ãƒãƒ¼ã®Google News RSSã‚’ä½¿ç”¨ã—ã¦ç”ŸæˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã€ŒğŸ¤–ï½œç”ŸæˆAIã€ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã«æŠ•ç¨¿

Google News RSSã‚’ä½¿ç”¨ï¼ˆå®Œå…¨ç„¡æ–™ãƒ»API keyä¸è¦ï¼‰
ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«ID: 1448462407763628274
"""

import os
import asyncio
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import discord
from discord.ext import commands
import aiohttp
import feedparser
import re
from html import unescape

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥èª­ã¿è¾¼ã¿
env_vars = {}
env_path = '/Users/minamitakeshi/discord-mcp-server/.env'
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                env_vars[key] = value

DISCORD_TOKEN = env_vars.get("DISCORD_TOKEN")

# ğŸ¤–ï½œç”ŸæˆAI ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«IDï¼ˆIdiomã‚µãƒ¼ãƒãƒ¼ï¼‰
GENAI_FORUM_ID = 1448462407763628274

# æŠ•ç¨¿å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆIdiomå°‚ç”¨ï¼‰
HISTORY_FILE = '/Users/minamitakeshi/discord-mcp-server/idiom_genai_news_history.json'

# Google News RSS URLï¼ˆæ—¥æœ¬èªã®ç”ŸæˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
GOOGLE_NEWS_RSS = 'https://news.google.com/rss/search?q=ç”ŸæˆAI OR ChatGPT OR Claude OR Gemini OR äººå·¥çŸ¥èƒ½ OR AI OR DeepMind OR OpenAI OR Anthropic&hl=ja&gl=JP&ceid=JP:ja'

# ä¿¡é ¼ã§ãã‚‹æ—¥æœ¬èªãƒ¡ãƒ‡ã‚£ã‚¢åã®ãƒªã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‚„summaryã«å«ã¾ã‚Œã‚‹æ–‡å­—åˆ—ã§åˆ¤å®šï¼‰
TRUSTED_MEDIA_NAMES = [
    'æ—¥æœ¬çµŒæ¸ˆæ–°è',
    'æ—¥çµŒ',
    'ITmedia',
    'CNET Japan',
    'CNET',
    'ã‚®ã‚ºãƒ¢ãƒ¼ãƒ‰',
    'Gizmodo',
    'WIRED',
    'Impress Watch',
    'PC Watch',
    'ASCII.jp',
    'ASCII',
    'ZDNet Japan',
    'ZDNet',
    'TechCrunch',
    'PR TIMES',
    'ãƒã‚¤ãƒŠãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹',
    'ãƒã‚¤ãƒŠãƒ“',
    'Engadget',
    'Notion',
    'Google',
    'OpenAI',
    'Anthropic',
]

# BotåˆæœŸåŒ–
intents = discord.Intents.default()
intents.guilds = True
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)


def load_history():
    """éå»ã®æŠ•ç¨¿å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€ï¼ˆç›´è¿‘30æ—¥åˆ†ï¼‰"""
    if not os.path.exists(HISTORY_FILE):
        return []

    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)

        # 30æ—¥ã‚ˆã‚Šå¤ã„å±¥æ­´ã‚’å‰Šé™¤ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰
        now = datetime.now(ZoneInfo('Asia/Tokyo'))
        cutoff_date = (now - timedelta(days=30)).isoformat()
        history = [item for item in history if item['date'] >= cutoff_date]

        return history
    except:
        return []


def save_history(history, new_items):
    """æŠ•ç¨¿å±¥æ­´ã‚’ä¿å­˜"""
    today = datetime.now(ZoneInfo('Asia/Tokyo')).isoformat()

    for item in new_items:
        history.append({
            'date': today,
            'title': item['title'],
            'url': item['url']
        })

    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def is_trusted_source(title, summary):
    """ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯è¦ç´„ã«ä¿¡é ¼ã§ãã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    combined_text = f"{title} {summary}"
    for media_name in TRUSTED_MEDIA_NAMES:
        if media_name in combined_text:
            return True
    return False


def is_duplicate(title, url, history):
    """å±¥æ­´ã¨é‡è¤‡ã—ã¦ã„ã‚‹ã‹ç¢ºèª"""
    for item in history:
        # URLãŒå®Œå…¨ä¸€è‡´
        if item['url'] == url:
            return True
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ãŒé«˜ã„ï¼ˆ80%ä»¥ä¸Šã®å˜èªä¸€è‡´ï¼‰
        title_words = set(title.split())
        history_words = set(item['title'].split())
        if title_words and history_words:
            similarity = len(title_words & history_words) / len(title_words | history_words)
            if similarity > 0.8:
                return True
    return False


async def fetch_google_news():
    """Google News RSSã‹ã‚‰æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(GOOGLE_NEWS_RSS, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    rss_content = await response.text()
                    return rss_content
                else:
                    print(f'RSSå–å¾—ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status}')
                    return None
    except Exception as e:
        print(f'RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}')
        return None


def remove_html_tags(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™"""
    if not text:
        return ''

    # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆ&lt; â†’ <ï¼‰
    text = unescape(text)

    # HTMLã‚¿ã‚°ã‚’å…¨ã¦é™¤å»
    text = re.sub(r'<[^>]+>', '', text)

    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'\s+', ' ', text)

    # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    text = text.strip()

    return text


def parse_rss(rss_content):
    """RSSã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    feed = feedparser.parse(rss_content)
    news_list = []

    # Google Newsã®è‡ªç„¶ãªé †åºã‚’ä¿¡é ¼ï¼ˆæ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãªã—ï¼‰
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆ30æ—¥å±¥æ­´ï¼‰ã§å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è‡ªå‹•çš„ã«é™¤å¤–ã•ã‚Œã‚‹
    for entry in feed.entries[:50]:  # æœ€æ–°50ä»¶ã¾ã§å–å¾—ï¼ˆã‚ˆã‚Šå¤šãã‹ã‚‰é¸åˆ¥ï¼‰
        # Google Newsã®RSSã¯entryã«title, link, published, summaryã‚’å«ã‚€
        title = entry.get('title', '')
        url = entry.get('link', '')
        summary = entry.get('summary', entry.get('description', ''))
        published = entry.get('published', '')

        # HTMLã‚¿ã‚°ã‚’é™¤å»
        title = remove_html_tags(title)
        summary = remove_html_tags(summary)

        # Google Newsã®ãƒªãƒ³ã‚¯ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«URLã‚’æŠ½å‡º
        # Google Newsã¯ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’ä½¿ã†ãŸã‚ã€å®Ÿéš›ã®URLã‚’å–å¾—
        if url:
            news_list.append({
                'title': title,
                'summary': summary[:200] if summary else 'ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¦‚è¦ãªã—',
                'url': url,
                'published': published
            })

    return news_list


async def verify_url(url):
    """URLãŒå®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹æ¤œè¨¼ã—ã€ã‚ªãƒªã‚¸ãƒŠãƒ«URLã‚’è¿”ã™"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.head(url, timeout=aiohttp.ClientTimeout(total=5), allow_redirects=True) as response:
                return (response.status < 400, str(response.url))
    except:
        # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸå ´åˆã€GETã§å†è©¦è¡Œ
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5), allow_redirects=True) as response:
                    return (response.status < 400, str(response.url))
        except:
            return (False, url)


@bot.event
async def on_ready():
    """Botèµ·å‹•æ™‚ã«å®Ÿè¡Œ"""
    print(f'Botèµ·å‹•: {bot.user}')

    try:
        # ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«ã‚’å–å¾—
        forum = bot.get_channel(GENAI_FORUM_ID)
        if not forum:
            print(f'ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«ï¼ˆID: {GENAI_FORUM_ID}ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
            await bot.close()
            return

        print(f'ãƒ•ã‚©ãƒ¼ãƒ©ãƒ : {forum.name}')

        # æŠ•ç¨¿å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€
        history = load_history()
        print(f'æŠ•ç¨¿å±¥æ­´: {len(history)}ä»¶')

        # Google News RSSã‹ã‚‰æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
        print('Google News RSSã‹ã‚‰ç”ŸæˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­...')
        rss_content = await fetch_google_news()

        if not rss_content:
            print('âŒ RSSå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ')
            await bot.close()
            return

        # RSSã‚’ãƒ‘ãƒ¼ã‚¹
        news_list = parse_rss(rss_content)
        print(f'RSSå–å¾—ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(news_list)}')

        if len(news_list) == 0:
            print('â„¹ï¸  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')
            await bot.close()
            return

        # å…ˆã«URLæ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¦ã‚ªãƒªã‚¸ãƒŠãƒ«URLã‚’å–å¾—ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ã®ç²¾åº¦å‘ä¸Šã®ãŸã‚ï¼‰
        print('URLæ¤œè¨¼ï¼†ã‚ªãƒªã‚¸ãƒŠãƒ«URLå–å¾—ä¸­...')
        verified_news = []
        for news in news_list:
            is_valid, original_url = await verify_url(news['url'])
            if is_valid:
                news['url'] = original_url  # ã‚ªãƒªã‚¸ãƒŠãƒ«URLã«ç½®ãæ›ãˆ
                verified_news.append(news)
                print(f'  âœ… URLæ¤œè¨¼OK: {original_url[:60]}...')
            else:
                print(f'  âŒ URLæ¤œè¨¼NGï¼ˆé™¤å¤–ï¼‰: {news["url"][:60]}...')

        if len(verified_news) == 0:
            print('âš ï¸  å…¨ã¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒURLæ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆæŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰')
            await bot.close()
            return

        print(f'æ¤œè¨¼æ¸ˆã¿ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(verified_news)}')

        # ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        print('ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...')
        trusted_news = []
        for news in verified_news:
            if is_trusted_source(news['title'], news['summary']):
                trusted_news.append(news)
                print(f'  âœ… ä¿¡é ¼ã§ãã‚‹æƒ…å ±æº: {news["title"][:50]}...')
            else:
                print(f'  â­ï¸  é™¤å¤–ï¼ˆä¿¡é ¼ã§ããªã„æƒ…å ±æºï¼‰: {news["title"][:50]}...')

        if len(trusted_news) == 0:
            print('â„¹ï¸  ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‹ã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆæŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰')
            await bot.close()
            return

        print(f'ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(trusted_news)}')

        # ã‚ªãƒªã‚¸ãƒŠãƒ«URLã§é‡è¤‡ã‚’é™¤å¤–
        print('é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«URLãƒ™ãƒ¼ã‚¹ï¼‰...')
        unique_news = []
        for news in trusted_news:
            if not is_duplicate(news['title'], news['url'], history):
                unique_news.append(news)
                print(f'  âœ… æ–°è¦: {news["title"][:50]}...')
            else:
                print(f'  â­ï¸  é‡è¤‡ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {news["title"][:50]}...')

        if len(unique_news) == 0:
            print('â„¹ï¸  æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆæŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰')
            await bot.close()
            return

        print(f'æ–°è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(unique_news)}')

        # æœ€æ–°2ä»¶ã®ã¿ã«çµã‚‹ï¼ˆ1æ™‚é–“ã”ã¨å®Ÿè¡Œã®ãŸã‚ï¼‰
        unique_news = unique_news[:2]
        print(f'æŠ•ç¨¿å¯¾è±¡: {len(unique_news)}ä»¶')

        # å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å€‹åˆ¥ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã¨ã—ã¦æŠ•ç¨¿
        now = datetime.now(ZoneInfo('Asia/Tokyo'))
        today = now.strftime('%Yå¹´%mæœˆ%dæ—¥')
        posted_count = 0

        for i, news in enumerate(unique_news, 1):
            thread_title = f"{news['title']}"

            # å®Œå…¨ã«ã‚·ãƒ³ãƒ—ãƒ«ãªå½¢å¼ï¼šè¦ç´„ + ç©ºè¡Œ + URL ã®ã¿
            thread_content = f"{news['summary']}\n\n{news['url']}"

            print(f'ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆä¸­ ({i}/{len(unique_news)}): {thread_title[:50]}...')

            thread = await forum.create_thread(
                name=thread_title[:100],  # Discordã®ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—æ•°åˆ¶é™å¯¾ç­–
                content=thread_content
            )

            print(f'  âœ… æŠ•ç¨¿å®Œäº†: {thread.thread.jump_url}')
            posted_count += 1

            # é€£ç¶šæŠ•ç¨¿ã®é–“éš”ã‚’ç©ºã‘ã‚‹
            await asyncio.sleep(2)

        print(f'\nâœ… å…¨{posted_count}ä»¶ã®æŠ•ç¨¿å®Œäº†')

        # æŠ•ç¨¿å±¥æ­´ã‚’ä¿å­˜
        save_history(history, unique_news)
        print('æŠ•ç¨¿å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ')

        # macOSé€šçŸ¥
        os.system(f'osascript -e \'display notification "{today}ã®ç”ŸæˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹ {posted_count}ä»¶ã‚’Idiomã«æŠ•ç¨¿ã—ã¾ã—ãŸ" with title "Idiomç”ŸæˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹"\'')

    except Exception as e:
        print(f'ã‚¨ãƒ©ãƒ¼: {e}')
        import traceback
        traceback.print_exc()

    await bot.close()


if __name__ == "__main__":
    bot.run(DISCORD_TOKEN)
