#!/usr/bin/env python3
"""
æ¯æ™‚Idiomã‚µãƒ¼ãƒãƒ¼ã®Google News RSSã‚’ä½¿ç”¨ã—ã¦åŒ»ç™‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã€ŒğŸ¥ï½œåŒ»ç™‚ã€ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã«æŠ•ç¨¿

Google News RSSã‚’ä½¿ç”¨ï¼ˆå®Œå…¨ç„¡æ–™ãƒ»API keyä¸è¦ï¼‰
ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«ID: 1452665608922730638
"""

import os
import asyncio
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import discord
from discord.ext import commands
import aiohttp
import feedparser
import re
from html import unescape

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥èª­ã¿è¾¼ã¿
env_vars = {}
env_path = '/Users/minamitakeshi/discord-mcp-server/.env'
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                env_vars[key] = value

DISCORD_TOKEN = env_vars.get("DISCORD_TOKEN")

# ğŸ¥ï½œåŒ»ç™‚ ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«IDï¼ˆIdiomã‚µãƒ¼ãƒãƒ¼ï¼‰
FORUM_ID = 1452665608922730638

# æŠ•ç¨¿å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆIdiomå°‚ç”¨ï¼‰
HISTORY_FILE = '/Users/minamitakeshi/discord-mcp-server/idiom_medical_news_history.json'

# Google News RSS URLï¼ˆåŒ»ç™‚é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ - æ²»ç™‚ãƒ»æ–°è–¬é–‹ç™ºã«ç‰¹åŒ–ï¼‰
# ã‚«ãƒ†ã‚´ãƒª: æ–°è–¬æ‰¿èªã€æ²»ç™‚æ³•é–‹ç™ºã€è‡¨åºŠè©¦é¨“æˆåŠŸã€ãŒã‚“æ²»ç™‚ã€å†ç”ŸåŒ»ç™‚
GOOGLE_NEWS_RSS = 'https://news.google.com/rss/search?q="æ–°è–¬æ‰¿èª" OR "æ²»ç™‚æ³•é–‹ç™º" OR "è‡¨åºŠè©¦é¨“ æˆåŠŸ" OR "æ²»é¨“ æˆåŠŸ" OR "åŠ¹æœç¢ºèª" OR "ç‰¹åŠ¹è–¬" OR "ç”»æœŸçš„æ²»ç™‚" OR "ãŒã‚“æ²»ç™‚ æ–°è–¬" OR "ãŒã‚“ æ–°è–¬" OR "ãŒã‚“ æ²»é¨“" OR "ãŒã‚“ æ‰¿èª" OR "å…ç–«ç™‚æ³• åŠ¹æœ" OR "éºä¼å­æ²»ç™‚ æˆåŠŸ" OR "å†ç”ŸåŒ»ç™‚ å®Ÿç”¨åŒ–" OR "iPSç´°èƒ æ²»ç™‚" OR "CAR-Tç™‚æ³•" OR "é›£ç—… æ²»ç™‚æ³•" OR "å¸Œå°‘ç–¾æ‚£ æ–°è–¬" OR "FDAæ‰¿èª æ–°è–¬" OR "åšåŠ´çœæ‰¿èª æ–°è–¬" OR "ä¸–ç•Œåˆ æ²»ç™‚" OR "æ²»é¨“é–‹å§‹" OR "æ‰‹è¡“æˆåŠŸ" OR "ç§»æ¤æˆåŠŸ"&hl=ja&gl=JP&ceid=JP:ja'

# ä¿¡é ¼ã§ãã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢åã®ãƒªã‚¹ãƒˆï¼ˆåŒ»ç™‚ç³»ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’è¿½åŠ ï¼‰
TRUSTED_MEDIA_NAMES = [
    # å¤§æ‰‹æ–°èãƒ»é€šä¿¡ç¤¾
    'æ—¥æœ¬çµŒæ¸ˆæ–°è',
    'æ—¥çµŒ',
    'æœæ—¥æ–°è',
    'èª­å£²æ–°è',
    'æ¯æ—¥æ–°è',
    'ç”£çµŒæ–°è',
    'æ™‚äº‹é€šä¿¡',
    'å…±åŒé€šä¿¡',
    # ãƒ†ãƒ¬ãƒ“
    'NHK',
    'TBS',
    'ãƒ†ãƒ¬ãƒ“æœæ—¥',
    'æ—¥æœ¬ãƒ†ãƒ¬ãƒ“',
    'ãƒ•ã‚¸ãƒ†ãƒ¬ãƒ“',
    'ãƒ†ãƒ¬ãƒ“æ±äº¬',
    # ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆãƒ¡ãƒ‡ã‚£ã‚¢
    'æ±æ´‹çµŒæ¸ˆ',
    'ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰',
    'Bloomberg',
    'ãƒ–ãƒ«ãƒ¼ãƒ ãƒãƒ¼ã‚°',
    'Reuters',
    'ãƒ­ã‚¤ã‚¿ãƒ¼',
    # åŒ»ç™‚ãƒ»ç§‘å­¦å°‚é–€ãƒ¡ãƒ‡ã‚£ã‚¢
    'æ—¥çµŒãƒ¡ãƒ‡ã‚£ã‚«ãƒ«',
    'æ—¥çµŒãƒã‚¤ã‚ªãƒ†ã‚¯',
    'Medical Tribune',
    'ãƒ¡ãƒ‡ã‚£ã‚«ãƒ«ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒ³',
    'AnswersNews',
    'ã‚¢ãƒ³ã‚µãƒ¼ã‚ºãƒ‹ãƒ¥ãƒ¼ã‚¹',
    'ãƒŸã‚¯ã‚¹Online',
    'MIXONLINE',
    'è–¬äº‹æ—¥å ±',
    'åŒ–å­¦å·¥æ¥­æ—¥å ±',
    'CareNet',
    'ã‚±ã‚¢ãƒãƒƒãƒˆ',
    'm3.com',
    'ã‚¨ãƒ ã‚¹ãƒªãƒ¼',
    'QLifePro',
    'ã‚­ãƒ¥ãƒ¼ãƒ©ã‚¤ãƒ•ãƒ—ãƒ­',
    'Medical DOC',
    'ãƒ¡ãƒ‡ã‚£ã‚«ãƒ«ãƒ‰ãƒƒã‚¯',
    'Nature',
    'ãƒã‚¤ãƒãƒ£ãƒ¼',
    'Science',
    'ã‚µã‚¤ã‚¨ãƒ³ã‚¹',
    'Lancet',
    'ãƒ©ãƒ³ã‚»ãƒƒãƒˆ',
    'NEJM',
    'New England Journal',
    'Cell',
    # è£½è–¬ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹
    'PR TIMES',
    'è£½è–¬å”',
    # ãã®ä»–ä¿¡é ¼ãƒ¡ãƒ‡ã‚£ã‚¢
    'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
    'ç¾ä»£ãƒ“ã‚¸ãƒã‚¹',
    'JBpress',
    'AERA',
    'é€±åˆŠãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰',
    'é€±åˆŠæ±æ´‹çµŒæ¸ˆ',
]

# BotåˆæœŸåŒ–
intents = discord.Intents.default()
intents.guilds = True
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)


def load_history():
    """éå»ã®æŠ•ç¨¿å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€ï¼ˆç›´è¿‘30æ—¥åˆ†ï¼‰"""
    if not os.path.exists(HISTORY_FILE):
        return []

    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)

        now = datetime.now(ZoneInfo('Asia/Tokyo'))
        cutoff_date = (now - timedelta(days=30)).isoformat()
        history = [item for item in history if item['date'] >= cutoff_date]

        return history
    except:
        return []


def save_history(history, new_items):
    """æŠ•ç¨¿å±¥æ­´ã‚’ä¿å­˜"""
    today = datetime.now(ZoneInfo('Asia/Tokyo')).isoformat()

    for item in new_items:
        history.append({
            'date': today,
            'title': item['title'],
            'url': item['url']
        })

    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def is_trusted_source(title, summary):
    """ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯è¦ç´„ã«ä¿¡é ¼ã§ãã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    combined_text = f"{title} {summary}"
    for media_name in TRUSTED_MEDIA_NAMES:
        if media_name in combined_text:
            return True
    return False


def is_duplicate(title, url, history):
    """å±¥æ­´ã¨é‡è¤‡ã—ã¦ã„ã‚‹ã‹ç¢ºèª"""
    for item in history:
        if item['url'] == url:
            return True
        title_words = set(title.split())
        history_words = set(item['title'].split())
        if title_words and history_words:
            similarity = len(title_words & history_words) / len(title_words | history_words)
            if similarity > 0.8:
                return True
    return False


async def fetch_google_news():
    """Google News RSSã‹ã‚‰æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(GOOGLE_NEWS_RSS, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    rss_content = await response.text()
                    return rss_content
                else:
                    print(f'RSSå–å¾—ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status}')
                    return None
    except Exception as e:
        print(f'RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}')
        return None


def remove_html_tags(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™"""
    if not text:
        return ''
    text = unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text


def parse_rss(rss_content):
    """RSSã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    feed = feedparser.parse(rss_content)
    news_list = []

    for entry in feed.entries[:50]:
        title = entry.get('title', '')
        url = entry.get('link', '')
        summary = entry.get('summary', entry.get('description', ''))
        published = entry.get('published', '')

        title = remove_html_tags(title)
        summary = remove_html_tags(summary)

        if url:
            news_list.append({
                'title': title,
                'summary': summary[:200] if summary else 'ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¦‚è¦ãªã—',
                'url': url,
                'published': published
            })

    return news_list


async def verify_url(url):
    """URLãŒå®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹æ¤œè¨¼ã—ã€ã‚ªãƒªã‚¸ãƒŠãƒ«URLã‚’è¿”ã™"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.head(url, timeout=aiohttp.ClientTimeout(total=5), allow_redirects=True) as response:
                return (response.status < 400, str(response.url))
    except:
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5), allow_redirects=True) as response:
                    return (response.status < 400, str(response.url))
        except:
            return (False, url)


@bot.event
async def on_ready():
    """Botèµ·å‹•æ™‚ã«å®Ÿè¡Œ"""
    print(f'Botèµ·å‹•: {bot.user}')

    try:
        forum = bot.get_channel(FORUM_ID)
        if not forum:
            print(f'ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«ï¼ˆID: {FORUM_ID}ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
            await bot.close()
            return

        print(f'ãƒ•ã‚©ãƒ¼ãƒ©ãƒ : {forum.name}')

        history = load_history()
        print(f'æŠ•ç¨¿å±¥æ­´: {len(history)}ä»¶')

        print('Google News RSSã‹ã‚‰åŒ»ç™‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­...')
        rss_content = await fetch_google_news()

        if not rss_content:
            print('âŒ RSSå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ')
            await bot.close()
            return

        news_list = parse_rss(rss_content)
        print(f'RSSå–å¾—ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(news_list)}')

        if len(news_list) == 0:
            print('â„¹ï¸  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')
            await bot.close()
            return

        print('URLæ¤œè¨¼ï¼†ã‚ªãƒªã‚¸ãƒŠãƒ«URLå–å¾—ä¸­...')
        verified_news = []
        for news in news_list:
            is_valid, original_url = await verify_url(news['url'])
            if is_valid:
                news['url'] = original_url
                verified_news.append(news)

        if len(verified_news) == 0:
            print('âš ï¸  å…¨ã¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒURLæ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ')
            await bot.close()
            return

        print(f'æ¤œè¨¼æ¸ˆã¿ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(verified_news)}')

        print('ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...')
        trusted_news = [news for news in verified_news if is_trusted_source(news['title'], news['summary'])]

        if len(trusted_news) == 0:
            print('â„¹ï¸  ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‹ã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')
            await bot.close()
            return

        print(f'ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(trusted_news)}')

        print('é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...')
        unique_news = [news for news in trusted_news if not is_duplicate(news['title'], news['url'], history)]

        if len(unique_news) == 0:
            print('â„¹ï¸  æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“')
            await bot.close()
            return

        print(f'æ–°è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(unique_news)}')

        unique_news = unique_news[:2]
        print(f'æŠ•ç¨¿å¯¾è±¡: {len(unique_news)}ä»¶')

        now = datetime.now(ZoneInfo('Asia/Tokyo'))
        today = now.strftime('%Yå¹´%mæœˆ%dæ—¥')
        posted_count = 0

        for i, news in enumerate(unique_news, 1):
            thread_title = f"{news['title']}"
            thread_content = f"{news['summary']}\n\n{news['url']}"

            print(f'ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆä¸­ ({i}/{len(unique_news)}): {thread_title[:50]}...')

            thread = await forum.create_thread(
                name=thread_title[:100],
                content=thread_content
            )

            print(f'  âœ… æŠ•ç¨¿å®Œäº†: {thread.thread.jump_url}')
            posted_count += 1

            await asyncio.sleep(2)

        print(f'\nâœ… å…¨{posted_count}ä»¶ã®æŠ•ç¨¿å®Œäº†')

        save_history(history, unique_news)
        print('æŠ•ç¨¿å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ')

        os.system(f'osascript -e \'display notification "{today}ã®åŒ»ç™‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ {posted_count}ä»¶ã‚’Idiomã«æŠ•ç¨¿ã—ã¾ã—ãŸ" with title "Idiom åŒ»ç™‚ãƒ‹ãƒ¥ãƒ¼ã‚¹"\'')

    except Exception as e:
        print(f'ã‚¨ãƒ©ãƒ¼: {e}')
        import traceback
        traceback.print_exc()

    await bot.close()


if __name__ == "__main__":
    bot.run(DISCORD_TOKEN)
